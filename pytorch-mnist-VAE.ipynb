{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Clarkdrengen/PML_2023/blob/main/pytorch-mnist-VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "NZkUZwpQaPyJ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Import necessary libraries to create a variational autoencoder\n",
        "The code is mainly developed using the PyTorch library\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "8HiWZxh5aPyM",
        "outputId": "f576ee0f-d901-4ea0-ec51-bab7e8433ae8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Determine if any GPUs are available \n",
        "\"\"\"\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "tgbqezBzJw6b",
        "outputId": "172c8e8b-8ff7-42d6-8981-a88415e21526",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# set the current working directory\n",
        "path = '/content/drive/MyDrive/PML_2023/'\n",
        "os.chdir(path)"
      ],
      "metadata": {
        "id": "0PxGbwC1Jpvs"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "yXIC0Z0naPyN"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Initialize Hyperparameters\n",
        "\"\"\"\n",
        "batch_size = 128\n",
        "learning_rate = 1e-3\n",
        "num_epochs = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "nvJRXVPgaPyN"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Create dataloaders to feed data into the neural network\n",
        "Default MNIST dataset is used and standard train/test split is performed\n",
        "\"\"\"\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('data', train=True, download=True,\n",
        "                    transform=transforms.ToTensor()),\n",
        "    batch_size=batch_size, shuffle=True)\n",
        "#    batch_size=1, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('data', train=False, transform=transforms.ToTensor()),\n",
        "    batch_size=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "1Wo5cV7vaPyO"
      },
      "outputs": [],
      "source": [
        "#A Convolutional Variational Autoencoder\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, imgChannels=1, featureDim=32*20*20, zDim=256):\n",
        "        super(VAE, self).__init__()\n",
        "        \n",
        "        # Initializing the 2 convolutional layers and 2 fully-connected layers for the encoder\n",
        "        self.encConv1 = nn.Conv2d(imgChannels, 16, 5)\n",
        "        self.encConv2 = nn.Conv2d(16, 32, 5) \n",
        "        self.encFC1 = nn.Linear(featureDim, zDim)\n",
        "        self.encFC2 = nn.Linear(featureDim, zDim)\n",
        "        \n",
        "        # Initializing the fully-connected layer and 2 convolutional layers for decoder\n",
        "        self.decFC1 = nn.Linear(zDim, featureDim)\n",
        "        self.decConv1 = nn.ConvTranspose2d(32, 16, 5)\n",
        "        self.decConv2 = nn.ConvTranspose2d(16, imgChannels, 5)\n",
        "        \n",
        "    def encoder(self, x):\n",
        "        \n",
        "        # Input is fed into 2 convolutional layers sequentially\n",
        "        # The output feature map are fed into 2 fully-connected layers to predict mean (mu) and variance (logVar)\n",
        "        # Mu and logVar are used for generating middle representation z and KL divergence loss\n",
        "        x = F.relu(self.encConv1(x))\n",
        "        x = F.relu(self.encConv2(x))\n",
        "        x = x.view(-1, 32*20*20)\n",
        "        mu = self.encFC1(x)\n",
        "        logVar = self.encFC2(x)\n",
        "        return mu, logVar\n",
        "    \n",
        "    def reparameterize(self, mu, logVar):\n",
        "        \n",
        "        #Reparameterization takes in the input mu and logVar and sample the mu + std * eps\n",
        "        std = torch.exp(logVar/2)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + std * eps\n",
        "    \n",
        "    def decoder(self, z):\n",
        "        \n",
        "        # z is fed back into a fully-connected layers and then into two transpose convolutional layers\n",
        "        # The generated output is the same size of the original input\n",
        "        x = F.relu(self.decFC1(z))\n",
        "        x = x.view(-1, 32, 20, 20)\n",
        "        x = F.relu(self.decConv1(x))\n",
        "        x = torch.sigmoid(self.decConv2(x))\n",
        "        return x\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        # The entire pipeline of the VAE: encoder -> reparameterization -> decoder\n",
        "        # output, mu, and logVar are returned for loss computation\n",
        "        mu, logVar = self.encoder(x)\n",
        "        z = self.reparameterize(mu, logVar)\n",
        "        out = self.decoder(z)\n",
        "        return out, mu, logVar\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "tuKXhmnnaPyV"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Initialize the network and the Adam optimizer\n",
        "\"\"\"\n",
        "net = VAE().to(device)\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "jY4GpRGMaPyW",
        "outputId": "a1f0fc20-eac6-4bd6-b723-9198b7d331d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-9829b3d11c66>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch {}: Loss {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m                                                f\"but got {result}.\")\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 state_steps)\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m     func(params,\n\u001b[0m\u001b[1;32m    282\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    389\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Training the network for a given number of epochs\n",
        "The loss after every epoch is printed\n",
        "\"\"\"\n",
        "for epoch in range(num_epochs):\n",
        "    for idx, data in enumerate(train_loader, 0):\n",
        "        imgs, _ = data\n",
        "        imgs = imgs.to(device)\n",
        "        \n",
        "        # Feeding a batch of images into the network to obtain the output image, mu, and logVar\n",
        "        out, mu, logVar = net(imgs)\n",
        "        \n",
        "        # The loss is the BCE loss combined with the KL divergence to ensure the distribution is learnt\n",
        "        kl_divergence = -0.5 * torch.sum(1 + logVar - mu.pow(2) - logVar.exp())\n",
        "        loss = F.binary_cross_entropy(out, imgs, size_average=False) + kl_divergence\n",
        "        \n",
        "        # Backpropagation based on the loss\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    print('Epoch {}: Loss {}'.format(epoch, loss))\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.save(net, '20230418_ConvNet_cpu.pt')\n",
        "net = torch.load(path + '20230418_ConvNet_cpu.pt')"
      ],
      "metadata": {
        "id": "oPG81tUoezim"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikYHRcZLe98u",
        "outputId": "c1256182-7d1a-4fae-9f21-014a2e61e322"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20230415_ConvNet.pt    20230416_VAE.pt\t\tdata\n",
            "20230415_Diffusion.pt  20230418_ConvNet_cpu.pt\tmnist_data\n",
            "20230415_VAE.pt        20230418_ConvNet.pt\tsynthetic_dataloader.pth\n",
            "20230416_ConvNet.pt    ConvNet.pt\t\tsynthetic_loader.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "_lOoDQnqLzcI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8aeca66a-600b-4321-a432-8a781cdceb04"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "ZOa15XL7aPyX",
        "outputId": "79cd8dc5-866f-417c-fadc-8e5f372a5eb0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "\"\"\"\n",
        "The following part takes a random image from test loader to feed into the VAE.\n",
        "Both the original image and generated image from the distribution are shown.\n",
        "\"\"\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "net.eval()\n",
        "with torch.no_grad():\n",
        "    for data in random.sample(list(test_loader), 1):\n",
        "        imgs, _ = data\n",
        "        imgs = imgs.to(device)\n",
        "        img = np.transpose(imgs[0].cpu().numpy(), [1,2,0])\n",
        "        plt.subplot(121)\n",
        "        plt.imshow(np.squeeze(img))\n",
        "        out, mu, logVAR = net(imgs)\n",
        "        outimg = np.transpose(out[0].cpu().numpy(), [1,2,0])\n",
        "        plt.subplot(122)\n",
        "        plt.imshow(np.squeeze(outimg))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate RMSE and MLP classification accuracy"
      ],
      "metadata": {
        "id": "YBGtxjqtP0XK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consolidate train images + labels"
      ],
      "metadata": {
        "id": "lxjc_b9CvMWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "reload training set with batch size = 1\n",
        "\"\"\"\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('data', train=True, download=True,\n",
        "                    transform=transforms.ToTensor()),\n",
        "#    batch_size=batch_size, shuffle=True)\n",
        "    batch_size=1, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('data', train=False, transform=transforms.ToTensor()),\n",
        "    batch_size=1)"
      ],
      "metadata": {
        "id": "KelnTQPjX8El"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = []\n",
        "for batch in train_loader:\n",
        "    images.append(batch[0])\n",
        "train_images = torch.cat(images, dim=0)\n",
        "\n",
        "train_labels = []\n",
        "for _, label in train_loader:\n",
        "    train_labels.append(label)"
      ],
      "metadata": {
        "id": "griNtIhvvOOP"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xf_2wLHb6OLF",
        "outputId": "d0af69ce-d723-4d0d-f3c9-adde7df69293"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = torch.as_tensor(train_labels, dtype=torch.float)\n",
        "print(train_labels.dtype)\n",
        "print(len(list(train_labels)))"
      ],
      "metadata": {
        "id": "VyRYejAd3PMP",
        "outputId": "4bbcd4b8-3fc7-4124-efce-4b7d4e0c726e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float32\n",
            "60000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consolidate test images + labels"
      ],
      "metadata": {
        "id": "qYatNT7-QAd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images = []\n",
        "for batch in test_loader:\n",
        "    images.append(batch[0])\n",
        "test_images = torch.cat(images, dim=0)\n",
        "\n",
        "test_labels = []\n",
        "for _, label in test_loader:\n",
        "    test_labels.append(label)\n",
        "test_labels = torch.as_tensor(test_labels, dtype=torch.float)\n",
        "test_labels = test_labels.type(torch.LongTensor)   # casting to long\n",
        "print(len(test_labels))\n",
        "print(test_labels.size())\n",
        "print(test_labels.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiCNXKf6QahB",
        "outputId": "97cf4872-56f4-4c6e-9bc1-5c1718008d59"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n",
            "torch.Size([10000])\n",
            "torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Candidate Synthetic loader"
      ],
      "metadata": {
        "id": "4Zjz9CRaW29w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an empty tensor to store the synthetic images\n",
        "synthetic_images = torch.empty((len(test_loader), 1, 28, 28), dtype=torch.float32, device=device)\n",
        "\n",
        "# Evaluate the Conv Net VAE model on the test dataset and generate synthetic images\n",
        "net.eval()\n",
        "with torch.no_grad():\n",
        "    for i, (image, label) in enumerate(test_loader):\n",
        "        image = image.to(device)\n",
        "        synthetic_image = net(image)[0].squeeze()\n",
        "        synthetic_images[i] = synthetic_image\n",
        "\n",
        "# h/t https://machinelearningmastery.com/training-a-pytorch-model-with-dataloader-and-dataset/\n",
        "# set up DataLoader for training set\n",
        "synthetic_loader = DataLoader(list(zip(synthetic_images, test_labels)), shuffle=False, batch_size=100)\n",
        "torch.save(synthetic_loader, \"20230419_synthetic_ConvNet_VAE_loader.pth\")\n"
      ],
      "metadata": {
        "id": "oh2O3785W4OC"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create Feed-forward Neural Network for supervised training"
      ],
      "metadata": {
        "id": "ca1B-xx50gZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network architecture\n",
        "class MNISTFeedForward(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNISTFeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        #x = torch.log_softmax(self.fc3(x), dim=0)\n",
        "        return x"
      ],
      "metadata": {
        "id": "cOzIIMhbNdNg"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Define hyperparameters\n",
        "batch_size = 64\n",
        "epochs =30\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load the MNIST dataset\n",
        "transform = transforms.ToTensor()\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "tZ60BZ1XNdPX"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model, loss function, and optimizer\n",
        "model = MNISTFeedForward()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "cGTYB14l0TZj"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#synthetic_loader = torch.load(\"synthetic_ConvNet_VAE_loader.pth\")"
      ],
      "metadata": {
        "id": "VvGlfjrS0TcG"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "for epoch in range(epochs):\n",
        "    for batch_idx, (data, target) in enumerate(synthetic_loader):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print('Epoch [{} / {}], Loss: {:.4f}'.format(epoch + 1, epochs, loss.item()))"
      ],
      "metadata": {
        "id": "PWqiHWjc0Te6",
        "outputId": "10047fd2-84d6-42db-d40f-6d12d78e45f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1 / 30], Loss: 0.5240\n",
            "Epoch [2 / 30], Loss: 0.4514\n",
            "Epoch [3 / 30], Loss: 0.4447\n",
            "Epoch [4 / 30], Loss: 0.4245\n",
            "Epoch [5 / 30], Loss: 0.3807\n",
            "Epoch [6 / 30], Loss: 0.3269\n",
            "Epoch [7 / 30], Loss: 0.2748\n",
            "Epoch [8 / 30], Loss: 0.2269\n",
            "Epoch [9 / 30], Loss: 0.1819\n",
            "Epoch [10 / 30], Loss: 0.1503\n",
            "Epoch [11 / 30], Loss: 0.1236\n",
            "Epoch [12 / 30], Loss: 0.0977\n",
            "Epoch [13 / 30], Loss: 0.0796\n",
            "Epoch [14 / 30], Loss: 0.0684\n",
            "Epoch [15 / 30], Loss: 0.0589\n",
            "Epoch [16 / 30], Loss: 0.0441\n",
            "Epoch [17 / 30], Loss: 0.0305\n",
            "Epoch [18 / 30], Loss: 0.0328\n",
            "Epoch [19 / 30], Loss: 0.0213\n",
            "Epoch [20 / 30], Loss: 0.0208\n",
            "Epoch [21 / 30], Loss: 0.0172\n",
            "Epoch [22 / 30], Loss: 0.0144\n",
            "Epoch [23 / 30], Loss: 0.0099\n",
            "Epoch [24 / 30], Loss: 0.0077\n",
            "Epoch [25 / 30], Loss: 0.0081\n",
            "Epoch [26 / 30], Loss: 0.0064\n",
            "Epoch [27 / 30], Loss: 0.0059\n",
            "Epoch [28 / 30], Loss: 0.0053\n",
            "Epoch [29 / 30], Loss: 0.0048\n",
            "Epoch [30 / 30], Loss: 0.0061\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, target in train_loader:\n",
        "        output = model(data)\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print('Accuracy on test dataset (called training): {:.2f}%'.format(accuracy))"
      ],
      "metadata": {
        "id": "NojwRV5P0TiE",
        "outputId": "2750aef4-ff8a-4e4f-ac39-d45f6c2f13f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test dataset (called training): 94.23%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FG86fzJw0Tj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7ldfkcYoNdRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ivhHXOYYNdTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train model"
      ],
      "metadata": {
        "id": "8wB-0RYWljRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the MLP model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Create an instance of the MLP model and move it to the selected device\n",
        "model = MLP().to(device)"
      ],
      "metadata": {
        "id": "aw9wANjhlc4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss() # applies a softmax funtion to the output layer and then calculates the log loss.\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "n_epochs = 10\n",
        "model.train()\n",
        "for epoch in range(n_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "      labels = labels.type(torch.LongTensor)   # casting to long\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(images)\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      if (i+1) % 5000 == 0:\n",
        "        print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                  .format(epoch+1, n_epochs, i+1, len(train_loader), loss.item()))\n"
      ],
      "metadata": {
        "id": "SOu0senNllkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate artificial model accuracy"
      ],
      "metadata": {
        "id": "0eUYrh94QUsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.__class__"
      ],
      "metadata": {
        "id": "U5PkEfAty-jB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize lists to monitor test loss and accuracy\n",
        "test_loss = 0.0\n",
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "model = model.to(device)\n",
        "\n",
        "model.eval() # prep model for *evaluation*\n",
        "\n",
        "for data, target in train_loader:\n",
        "    # forward pass: compute predicted outputs by passing inputs to the model\n",
        "    output = model(data)\n",
        "    # calculate the loss\n",
        "    loss = criterion(output, target)\n",
        "    # update test loss \n",
        "    test_loss += loss.item()*data.size(0)\n",
        "    # convert output probabilities to predicted class\n",
        "    _, pred = torch.max(output, 1)\n",
        "    # compare predictions to true label\n",
        "    correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
        "    # calculate test accuracy for each object class\n",
        "    for i in range(batch_size):\n",
        "        label = target.data[i]\n",
        "        class_correct[label] += correct[i].item()\n",
        "        class_total[label] += 1\n",
        "\n",
        "# calculate and print avg test loss\n",
        "test_loss = test_loss/len(test_loader.dataset)\n",
        "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "for i in range(10):\n",
        "    if class_total[i] > 0:\n",
        "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "            str(i), 100 * class_correct[i] / class_total[i],\n",
        "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "    else:\n",
        "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "\n",
        "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "    100. * np.sum(class_correct) / np.sum(class_total),\n",
        "    np.sum(class_correct), np.sum(class_total)))"
      ],
      "metadata": {
        "id": "a6ugfeBNx1km"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# evaluate accuracy after training\n",
        "model.eval()\n",
        "test_tensor = torch.tensor(train_images, dtype=torch.float32)\n",
        "test_tensor.size()\n",
        "\n",
        "#y_pred = model(test_tensor).squeeze().round()\n",
        "#acc = (y_pred == torch.tensor(train_labels, dtype=torch.float32)).float().mean().item()\n",
        "#print(\"Model accuracy: %.2f%%\" % (acc*100))"
      ],
      "metadata": {
        "id": "mImfwzCiQdvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load generic VAE for comparison"
      ],
      "metadata": {
        "id": "nTe-8S4gk3tM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDSaRpXDaPyY"
      },
      "outputs": [],
      "source": [
        "# prerequisites\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper-parameters\n",
        "image_size = 784\n",
        "bs = 128\n",
        "#h_dim = 400\n",
        "#z_dim = 20\n",
        "num_epochs = 50\n",
        "x_dim=784, \n",
        "h_dim1= 512, \n",
        "h_dim2=256, \n",
        "z_dim=2\n",
        "learning_rate = 1e-3"
      ],
      "metadata": {
        "id": "LaoGCObKk97H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MNIST Dataset\n",
        "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
        "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False)"
      ],
      "metadata": {
        "id": "2W496PmFMCbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim1, h_dim2, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        \n",
        "        # encoder part\n",
        "        self.fc1 = nn.Linear(x_dim, h_dim1)\n",
        "        self.fc2 = nn.Linear(h_dim1, h_dim2)\n",
        "        self.fc31 = nn.Linear(h_dim2, z_dim)\n",
        "        self.fc32 = nn.Linear(h_dim2, z_dim)\n",
        "        # decoder part\n",
        "        self.fc4 = nn.Linear(z_dim, h_dim2)\n",
        "        self.fc5 = nn.Linear(h_dim2, h_dim1)\n",
        "        self.fc6 = nn.Linear(h_dim1, x_dim)\n",
        "        \n",
        "    def encoder(self, x):\n",
        "        h = F.relu(self.fc1(x))\n",
        "        h = F.relu(self.fc2(h))\n",
        "        return self.fc31(h), self.fc32(h) # mu, log_var\n",
        "    \n",
        "    def sampling(self, mu, log_var):\n",
        "        std = torch.exp(0.5*log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        return eps.mul(std).add_(mu) # return z sample\n",
        "        \n",
        "    def decoder(self, z):\n",
        "        h = F.relu(self.fc4(z))\n",
        "        h = F.relu(self.fc5(h))\n",
        "        return F.sigmoid(self.fc6(h)) \n",
        "    \n",
        "    def forward(self, x):\n",
        "        mu, log_var = self.encoder(x.view(-1, 784))\n",
        "        z = self.sampling(mu, log_var)\n",
        "        return self.decoder(z), mu, log_var\n",
        "\n",
        "# build model\n",
        "#vae = VAE(x_dim=image_size , h_dim1= 512, h_dim2=256, z_dim=2)\n",
        "vae = VAE(x_dim=image_size , h_dim1= 512, h_dim2=256, z_dim=z_dim)\n",
        "if torch.cuda.is_available():\n",
        "    vae.cuda()"
      ],
      "metadata": {
        "id": "Nga64sE1lAjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vae.parameters"
      ],
      "metadata": {
        "id": "RCBc46hvlH6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(vae.parameters(), lr=learning_rate)\n",
        "# return reconstruction error + KL divergence losses\n",
        "def loss_function(recon_x, x, mu, log_var):\n",
        "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "    return BCE + KLD"
      ],
      "metadata": {
        "id": "XxbM7XXDlIeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    vae.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        if torch.cuda.is_available():\n",
        "          data = data.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        recon_batch, mu, log_var = vae(data)\n",
        "        loss = loss_function(recon_batch, data, mu, log_var)\n",
        "        \n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch_idx % 100 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item() / len(data)))\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))"
      ],
      "metadata": {
        "id": "ZiXHdpXHlMxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "    vae.eval()\n",
        "    test_loss= 0\n",
        "    with torch.no_grad():\n",
        "        for data, _ in test_loader:\n",
        "            if torch.cuda.is_available():\n",
        "              data = data.cuda()\n",
        "            recon, mu, log_var = vae(data)\n",
        "            \n",
        "            # sum up batch loss\n",
        "            test_loss += loss_function(recon, data, mu, log_var).item()\n",
        "        \n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('====> Test set loss: {:.4f}'.format(test_loss))"
      ],
      "metadata": {
        "id": "Rwn_fCbKlPym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, num_epochs + 1):\n",
        "    train(epoch)\n",
        "    test()"
      ],
      "metadata": {
        "id": "lmJ7507mlQZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(vae, '20230416_VAE.pt')\n",
        "#vae = torch.load(path + '20230415_VAE.pt')"
      ],
      "metadata": {
        "id": "TXHA9UmyLWNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# store ground truth test images\n",
        "test_images_ground_truth, _ = next(iter(test_loader))\n",
        "test_images_ground_truth = test_images_ground_truth[-12:,:,:,:] #grab 12 last images"
      ],
      "metadata": {
        "id": "hAFYZQdwsacb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## now create an image of estimated images and ground truth\n",
        "# h/t https://stackoverflow.com/questions/66667949/pytorch-mnist-autoencoder-to-learn-10-digit-classification\n",
        "\n",
        "## run first five training images through the encoder\n",
        "### from https://github.com/dataflowr/notebooks/blob/master/HW3/VAE_clustering_empty.ipynb\n",
        "\n",
        "def show(img):\n",
        "    npimg = img.cpu().numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
        "    plt.suptitle('Test set reconstruction VAE & ConvNet VAE', fontsize=10, y=.67)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "def plot_reconstruction(vae, n=12):\n",
        "    \n",
        "    x = test_images_ground_truth\n",
        "    x = x[:n,:,:,:].to(device)\n",
        "    print(\"Ground truth images reformatted are of dimension\", x.size())\n",
        "    try:\n",
        "        out, _, _, log_p = vae(x.view(-1, image_size)) \n",
        "    except:\n",
        "        out, _, _ = vae(x.view(-1, image_size))\n",
        "    x_concat = torch.cat([x.view(-1, 1, 28, 28), out.view(-1, 1, 28, 28)], dim=0)\n",
        "    print(\"Concatenated object containing both ground truth & generic VAE is of size\", out.size())\n",
        "    ##\n",
        "    out, mu, logVAR = net(x)\n",
        "    \n",
        "    \n",
        "    x_concat = torch.cat([x_concat, out.view(-1, 1, 28, 28)], dim=0)\n",
        "    print(\"Concatenated object also containing ConvNet VAE is of size\", x_concat.size())\n",
        "\n",
        "    print(out.size())\n",
        "    print(out.type())\n",
        "    ##\n",
        "    out_grid = torchvision.utils.make_grid(x_concat, nrow=12)#.cpu().data\n",
        "    show(out_grid)\n",
        "    print(x_concat.size())\n",
        "\n",
        "plot_reconstruction(vae)"
      ],
      "metadata": {
        "id": "dAbGnY4mxOKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239) for MNIST\n",
        "(J. Ho, A. Jain, P. Abbeel 2020)\n",
        "\n",
        "![](https://raw.githubusercontent.com/dataflowr/website/master/modules/extras/diffusions/ddpm.png)\n",
        "\n",
        "\n",
        "Given a schedule $\\beta_1<\\beta_2<\\dots <\\beta_T$, the **forward diffusion process** is defined by:\n",
        "$q(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t}x_{t-1},\\beta_t I)$ and $q(x_{1:T}|x_0) = \\prod_{t=1}^T q(x_t|x_{t-1})$.\n",
        "\n",
        "With $\\alpha_t = 1-\\beta_t$ and $\\overline{\\alpha_t} = \\prod_{i=1}^t\\alpha_i$, we see that, with $\\epsilon\\sim\\mathcal{N}(0,I)$:\n",
        "\\begin{align*}\n",
        "x_t = \\sqrt{\\overline{\\alpha}_t}x_0 + \\sqrt{1-\\overline{\\alpha}_t}\\epsilon.\n",
        "\\end{align*}\n",
        "The law $q(x_{t-1}|x_t,\\epsilon)$ is explicit: $q(x_{t-1}|x_t,\\epsilon) = \\mathcal{N}(x_{t-1};\\mu(x_t,\\epsilon,t), \\gamma_t I)$ with,\n",
        "\\begin{align*}\n",
        "\\mu(x_t,\\epsilon, t) = \\frac{1}{\\sqrt{\\alpha_t}}\\left( x_t-\\frac{1-\\alpha_t}{\\sqrt{1-\\overline{\\alpha}_t}}\\epsilon\\right)\\text{ and, }\n",
        "\\gamma_t = \\frac{1-\\overline{\\alpha}_{t-1}}{1-\\overline{\\alpha}_{t}}\\beta_t\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "**Training**: to approximate **the reversed diffusion** $q(x_{t-1}|x_t)$ by a neural network given by $p_{\\theta}(x_{t-1}|x_t) = \\mathcal{N}(x_{t-1}; \\mu_{\\theta}(x_t,t), \\beta_t I)$ and $p(x_T) \\sim \\mathcal{N}(0,I)$, we maximize the usual Variational bound:\n",
        "\\begin{align*}\n",
        "\\mathbb{E}_{q(x_0)} \\ln p_{\\theta}(x_0) &\\geq L_T +\\sum_{t=2}^T L_{t-1}+L_0 \\text{ with, }L_{t-1} = \\mathbb{E}_q\\left[ \\frac{1}{2\\sigma_t^2}\\|\\mu_\\theta(x_t,t) -\\mu(x_t,\\epsilon,t)\\|^2\\right].\n",
        "\\end{align*}\n",
        "With the change of variable:\n",
        "\\begin{align*}\n",
        "\\mu_\\theta(x_t,t) = \\frac{1}{\\sqrt{\\alpha_t}}\\left( x_t-\\frac{1-\\alpha_t}{\\sqrt{1-\\overline{\\alpha}_t}}\\epsilon_\\theta(x_t,t)\\right),\n",
        "\\end{align*}\n",
        "ignoring the prefactor and sampling $\\tau$ instead of summing over all $t$, the loss is finally:\n",
        "\\begin{align*}\n",
        "\\ell(\\theta) = \\mathbb{E}_\\tau\\mathbb{E}_\\epsilon \\left[ \\|\\epsilon - \\epsilon_\\theta(\\sqrt{\\overline{\\alpha}_\\tau}x_0 + \\sqrt{1-\\overline{\\alpha}_\\tau}\\epsilon, \\tau)\\|^2\\right]\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "\n",
        "**Sampling**: to simulate the reversed diffusion with the learned $\\epsilon_\\theta(x_t,t)$ starting from $x_T\\sim \\mathcal{N}(0,I)$, iterate for $t=T,\\dots, 1$:\n",
        "\\begin{align*}\n",
        "x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}}\\left( x_t-\\frac{1-\\alpha_t}{\\sqrt{1-\\overline{\\alpha}_t}}\\epsilon_\\theta(x_t,t)\\right)+\\sqrt{\\beta_t}\\epsilon,\\text{ with } \\epsilon\\sim\\mathcal{N}(0,I).\n",
        "\\end{align*}"
      ],
      "metadata": {
        "id": "j3AWs4yCtc_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "6MB01CZZuKKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_images(images, title=\"\"):\n",
        "    \"\"\"Shows the provided images as sub-pictures in a square\"\"\"\n",
        "    images = [im.permute(1,2,0).numpy() for im in images]\n",
        "\n",
        "    # Defining number of rows and columns\n",
        "    fig = plt.figure(figsize=(8, 8))\n",
        "    rows = int(len(images) ** (1 / 2))\n",
        "    cols = round(len(images) / rows)\n",
        "\n",
        "    # Populating figure with sub-plots\n",
        "    idx = 0\n",
        "    for r in range(rows):\n",
        "        for c in range(cols):\n",
        "            fig.add_subplot(rows, cols, idx + 1)\n",
        "\n",
        "            if idx < len(images):\n",
        "                plt.imshow(images[idx], cmap=\"gray\")\n",
        "                plt.axis('off')\n",
        "                idx += 1\n",
        "    fig.suptitle(title, fontsize=30)\n",
        "    \n",
        "    # Showing the figure\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "D1Sswopa4zp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sinusoidal_embedding(n, d):\n",
        "    # Returns the standard positional embedding\n",
        "    embedding = torch.tensor([[i / 10_000 ** (2 * j / d) for j in range(d)] for i in range(n)])\n",
        "    sin_mask = torch.arange(0, n, 2)\n",
        "\n",
        "    embedding[sin_mask] = torch.sin(embedding[sin_mask])\n",
        "    embedding[1 - sin_mask] = torch.cos(embedding[sin_mask])\n",
        "\n",
        "    return embedding"
      ],
      "metadata": {
        "id": "Qv7b155x42RA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyConv(nn.Module):\n",
        "    def __init__(self, shape, in_c, out_c, kernel_size=3, stride=1, padding=1, activation=None, normalize=True):\n",
        "        super(MyConv, self).__init__()\n",
        "        self.ln = nn.LayerNorm(shape)\n",
        "        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size, stride, padding)\n",
        "        self.activation = nn.SiLU() if activation is None else activation\n",
        "        self.normalize = normalize\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.ln(x) if self.normalize else x\n",
        "        out = self.conv1(out)\n",
        "        out = self.activation(out)\n",
        "        return out\n",
        "    \n",
        "def MyTinyBlock(size, in_c, out_c):\n",
        "    return nn.Sequential(MyConv((in_c, size, size), in_c, out_c), \n",
        "                         MyConv((out_c, size, size), out_c, out_c), \n",
        "                         MyConv((out_c, size, size), out_c, out_c))\n",
        "\n",
        "def MyTinyUp(size, in_c):\n",
        "    return nn.Sequential(MyConv((in_c, size, size), in_c, in_c//2), \n",
        "                         MyConv((in_c//2, size, size), in_c//2, in_c//4), \n",
        "                         MyConv((in_c//4, size, size), in_c//4, in_c//4))"
      ],
      "metadata": {
        "id": "2H_n9wrz42zW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyTinyUNet(nn.Module):\n",
        "  # Here is a network with 3 down and 3 up with the tiny block\n",
        "    def __init__(self, in_c=1, out_c=1, size=32, n_steps=1000, time_emb_dim=100):\n",
        "        super(MyTinyUNet, self).__init__()\n",
        "\n",
        "        # Sinusoidal embedding\n",
        "        self.time_embed = nn.Embedding(n_steps, time_emb_dim)\n",
        "        self.time_embed.weight.data = sinusoidal_embedding(n_steps, time_emb_dim)\n",
        "        self.time_embed.requires_grad_(False)\n",
        "\n",
        "        # First half\n",
        "        self.te1 = self._make_te(time_emb_dim, 1)\n",
        "        self.b1 = MyTinyBlock(size, in_c, 10)\n",
        "        self.down1 = nn.Conv2d(10, 10, 4, 2, 1)\n",
        "        self.te2 = self._make_te(time_emb_dim, 10)\n",
        "        self.b2 = MyTinyBlock(size//2, 10, 20)\n",
        "        self.down2 = nn.Conv2d(20, 20, 4, 2, 1)\n",
        "        self.te3 = self._make_te(time_emb_dim, 20)\n",
        "        self.b3 = MyTinyBlock(size//4, 20, 40)\n",
        "        self.down3 = nn.Conv2d(40, 40, 4, 2, 1)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.te_mid = self._make_te(time_emb_dim, 40)\n",
        "        self.b_mid = nn.Sequential(\n",
        "            MyConv((40, size//8, size//8), 40, 20),\n",
        "            MyConv((20, size//8, size//8), 20, 20),\n",
        "            MyConv((20, size//8, size//8), 20, 40)\n",
        "        )\n",
        "\n",
        "        # Second half\n",
        "        self.up1 = nn.ConvTranspose2d(40, 40, 4, 2, 1)\n",
        "        self.te4 = self._make_te(time_emb_dim, 80)\n",
        "        self.b4 = MyTinyUp(size//4, 80)\n",
        "        self.up2 = nn.ConvTranspose2d(20, 20, 4, 2, 1)\n",
        "        self.te5 = self._make_te(time_emb_dim, 40)\n",
        "        self.b5 = MyTinyUp(size//2, 40)\n",
        "        self.up3 = nn.ConvTranspose2d(10, 10, 4, 2, 1)\n",
        "        self.te_out = self._make_te(time_emb_dim, 20)\n",
        "        self.b_out = MyTinyBlock(size, 20, 10)\n",
        "        self.conv_out = nn.Conv2d(10, out_c, 3, 1, 1)\n",
        "\n",
        "    def forward(self, x, t): # x is (bs, in_c, size, size) t is (bs)\n",
        "        t = self.time_embed(t)\n",
        "        n = len(x)\n",
        "        out1 = self.b1(x + self.te1(t).reshape(n, -1, 1, 1))  # (bs, 10, size/2, size/2)\n",
        "        out2 = self.b2(self.down1(out1) + self.te2(t).reshape(n, -1, 1, 1))  # (bs, 20, size/4, size/4)\n",
        "        out3 = self.b3(self.down2(out2) + self.te3(t).reshape(n, -1, 1, 1))  # (bs, 40, size/8, size/8)\n",
        "\n",
        "        out_mid = self.b_mid(self.down3(out3) + self.te_mid(t).reshape(n, -1, 1, 1))  # (bs, 40, size/8, size/8)\n",
        "\n",
        "        out4 = torch.cat((out3, self.up1(out_mid)), dim=1)  # (bs, 80, size/8, size/8)\n",
        "        out4 = self.b4(out4 + self.te4(t).reshape(n, -1, 1, 1))  # (bs, 20, size/8, size/8)\n",
        "        out5 = torch.cat((out2, self.up2(out4)), dim=1)  # (bs, 40, size/4, size/4)\n",
        "        out5 = self.b5(out5 + self.te5(t).reshape(n, -1, 1, 1))  # (bs, 10, size/2, size/2)\n",
        "        out = torch.cat((out1, self.up3(out5)), dim=1)  # (bs, 20, size, size)\n",
        "        out = self.b_out(out + self.te_out(t).reshape(n, -1, 1, 1))  # (bs, 10, size, size)\n",
        "        out = self.conv_out(out) # (bs, out_c, size, size)\n",
        "        return out\n",
        "\n",
        "    def _make_te(self, dim_in, dim_out):\n",
        "        return nn.Sequential(nn.Linear(dim_in, dim_out), nn.SiLU(), nn.Linear(dim_out, dim_out))"
      ],
      "metadata": {
        "id": "EiGggoHH46UI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bs = 3\n",
        "x = torch.randn(bs,1,32,32)\n",
        "n_steps=1000\n",
        "timesteps = torch.randint(0, n_steps, (bs,)).long()\n",
        "unet = MyTinyUNet(in_c =1, out_c =1, size=32)"
      ],
      "metadata": {
        "id": "TcHLTuZs491H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = unet(x,timesteps)\n",
        "y.shape"
      ],
      "metadata": {
        "id": "AkzV3d6K5Bqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DDPM(nn.Module):\n",
        "    def __init__(self, network, num_timesteps, beta_start=0.0001, beta_end=0.02, device=device) -> None:\n",
        "        super(DDPM, self).__init__()\n",
        "        self.num_timesteps = num_timesteps\n",
        "        self.betas = torch.linspace(beta_start, beta_end, num_timesteps, dtype=torch.float32).to(device)\n",
        "        self.alphas = 1.0 - self.betas\n",
        "        self.alphas_cumprod = torch.cumprod(self.alphas, axis=0)\n",
        "        self.network = network\n",
        "        self.device = device\n",
        "        self.sqrt_alphas_cumprod = self.alphas_cumprod ** 0.5 # used in add_noise\n",
        "        self.sqrt_one_minus_alphas_cumprod = (1 - self.alphas_cumprod) ** 0.5 # used in add_noise and step\n",
        "\n",
        "    def add_noise(self, x_start, x_noise, timesteps):\n",
        "        # The forward process\n",
        "        # x_start and x_noise (bs, n_c, w, d)\n",
        "        # timesteps (bs)\n",
        "        s1 = self.sqrt_alphas_cumprod[timesteps] # bs\n",
        "        s2 = self.sqrt_one_minus_alphas_cumprod[timesteps] # bs\n",
        "        s1 = s1.reshape(-1,1,1,1) # (bs, 1, 1, 1) for broadcasting\n",
        "        s2 = s2.reshape(-1,1,1,1) # (bs, 1, 1, 1)\n",
        "        return s1 * x_start + s2 * x_noise\n",
        "\n",
        "    def reverse(self, x, t):\n",
        "        # The network return the estimation of the noise we added\n",
        "        return self.network(x, t)\n",
        "    \n",
        "    def step(self, model_output, timestep, sample):\n",
        "        # one step of sampling\n",
        "        # timestep (1)\n",
        "        t = timestep\n",
        "        coef_epsilon = (1-self.alphas)/self.sqrt_one_minus_alphas_cumprod\n",
        "        coef_eps_t = coef_epsilon[t].reshape(-1,1,1,1)\n",
        "        coef_first = 1/self.alphas ** 0.5\n",
        "        coef_first_t = coef_first[t].reshape(-1,1,1,1)\n",
        "        pred_prev_sample = coef_first_t*(sample-coef_eps_t*model_output)\n",
        "\n",
        "        variance = 0\n",
        "        if t > 0:\n",
        "            noise = torch.randn_like(model_output).to(self.device)\n",
        "            variance = ((self.betas[t] ** 0.5) * noise)\n",
        "            \n",
        "        pred_prev_sample = pred_prev_sample + variance\n",
        "\n",
        "        return pred_prev_sample"
      ],
      "metadata": {
        "id": "H3THr3215CJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_timesteps = 1000\n",
        "betas = torch.linspace(0.0001, 0.02, num_timesteps, dtype=torch.float32).to(device)"
      ],
      "metadata": {
        "id": "_AwkImC65GO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "betas[timesteps]"
      ],
      "metadata": {
        "id": "bgr-Umab5G1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "betas[10]"
      ],
      "metadata": {
        "id": "QcXUtiAw5KzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "betas[timesteps].reshape(-1,1,1,1).shape"
      ],
      "metadata": {
        "id": "U0D5vxS85MxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network = MyTinyUNet(in_c =1, out_c =1, size=32)\n",
        "model = DDPM(network, num_timesteps, beta_start=0.0001, beta_end=0.02, device=device)"
      ],
      "metadata": {
        "id": "zKy8r0b35PHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bs = 5\n",
        "x = torch.randn(bs,1,32,32).to(device)\n",
        "timesteps = 10*torch.ones(bs,).long().long().to(device)"
      ],
      "metadata": {
        "id": "QYzBel6p5Php"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timesteps.shape"
      ],
      "metadata": {
        "id": "6Y3wgiGx5Rdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = model.add_noise(x,x,timesteps)\n",
        "y.shape"
      ],
      "metadata": {
        "id": "nQdZixkS5XMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = model.step(x,timesteps[0],x)\n",
        "y.shape"
      ],
      "metadata": {
        "id": "fvAqNm5v5bM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can check that all the parameters of the UNet network are indeed parameters of the DDPM model like this:"
      ],
      "metadata": {
        "id": "ysEcs_Fc5eXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for n, p in model.named_parameters():\n",
        "    print(n, p.shape)"
      ],
      "metadata": {
        "id": "pQScyX495fMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(model, dataloader, optimizer, num_epochs, num_timesteps, device=device):\n",
        "    \"\"\"Training loop for DDPM\"\"\"\n",
        "\n",
        "    global_step = 0\n",
        "    losses = []\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        progress_bar = tqdm(total=len(dataloader))\n",
        "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
        "        for step, batch in enumerate(dataloader):\n",
        "            batch = batch[0].to(device)\n",
        "            noise = torch.randn(batch.shape).to(device)\n",
        "            timesteps = torch.randint(0, num_timesteps, (batch.shape[0],)).long().to(device)\n",
        "\n",
        "            noisy = model.add_noise(batch, noise, timesteps)\n",
        "            noise_pred = model.reverse(noisy, timesteps)\n",
        "            loss = F.mse_loss(noise_pred, noise)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            progress_bar.update(1)\n",
        "            logs = {\"loss\": loss.detach().item(), \"step\": global_step}\n",
        "            losses.append(loss.detach().item())\n",
        "            progress_bar.set_postfix(**logs)\n",
        "            global_step += 1\n",
        "        \n",
        "        progress_bar.close()"
      ],
      "metadata": {
        "id": "1mPyGVwQ5h3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir = './data/'\n",
        "transform01 = torchvision.transforms.Compose([\n",
        "        torchvision.transforms.Resize(32),\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize((0.5), (0.5))\n",
        "    ])\n",
        "dataset = torchvision.datasets.MNIST(root=root_dir, train=True, transform=transform01, download=True)\n",
        "dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=4096, shuffle=True, num_workers=10)"
      ],
      "metadata": {
        "id": "dCPEsBce5kEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for b in dataloader:\n",
        "    batch = b[0]\n",
        "    break\n",
        "\n",
        "bn = [b for b in batch[:100]] \n",
        "show_images(bn, \"origin\")"
      ],
      "metadata": {
        "id": "pNH9KpU65oVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "num_epochs = 50\n",
        "num_timesteps = 1000\n",
        "network = MyTinyUNet()\n",
        "network = network.to(device)\n",
        "model = DDPM(network, num_timesteps, beta_start=0.0001, beta_end=0.02, device=device)\n",
        "optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate)\n",
        "training_loop(model, dataloader, optimizer, num_epochs, num_timesteps, device=device)        "
      ],
      "metadata": {
        "id": "qG2L6O6L5quP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.save(model, '20230415_Diffusion.pt')\n",
        "model = torch.load(path + '20230416_Diffusion.pt')"
      ],
      "metadata": {
        "id": "kGf-u0qp4Ya5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_image(ddpm, sample_size, channel, size):\n",
        "    \"\"\"Generate the image from the Gaussian noise\"\"\"\n",
        "\n",
        "    frames = []\n",
        "    frames_mid = []\n",
        "    ddpm.eval()\n",
        "    with torch.no_grad():\n",
        "        timesteps = list(range(ddpm.num_timesteps))[::-1]\n",
        "        sample = torch.randn(sample_size, channel, size, size).to(device)\n",
        "        \n",
        "        for i, t in enumerate(tqdm(timesteps)):\n",
        "            time_tensor = (torch.ones(sample_size,1) * t).long().to(device)\n",
        "            residual = ddpm.reverse(sample, time_tensor)\n",
        "            sample = ddpm.step(residual, time_tensor[0], sample)\n",
        "\n",
        "            if t==500:\n",
        "                for i in range(sample_size):\n",
        "                    frames_mid.append(sample[i].detach().cpu())\n",
        "\n",
        "        for i in range(sample_size):\n",
        "            frames.append(sample[i].detach().cpu())\n",
        "    return frames, frames_mid"
      ],
      "metadata": {
        "id": "eQXWsjaG5s6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated, generated_mid = generate_image(model, 100, 1, 32)"
      ],
      "metadata": {
        "id": "jU97qOJi5vDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_images(generated_mid, \"Mid result\")\n",
        "show_images(generated, \"Final result\")"
      ],
      "metadata": {
        "id": "rcQMvn2s5xM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rescale(x):\n",
        "    return (x+1)/2\n",
        "\n",
        "def show_images_rescale(images, title=\"\"):\n",
        "    \"\"\"Shows the provided images as sub-pictures in a square\"\"\"\n",
        "    images = [rescale((im.permute(1,2,0)).numpy()) for im in images]\n",
        "\n",
        "    # Defining number of rows and columns\n",
        "    fig = plt.figure(figsize=(8, 8))\n",
        "    rows = int(len(images) ** (1 / 2))\n",
        "    cols = round(len(images) / rows)\n",
        "\n",
        "    # Populating figure with sub-plots\n",
        "    idx = 0\n",
        "    for r in range(rows):\n",
        "        for c in range(cols):\n",
        "            fig.add_subplot(rows, cols, idx + 1)\n",
        "\n",
        "            if idx < len(images):\n",
        "                #plt.imshow(images[idx].reshape(pixel, pixel, n_channels), cmap=\"gray\")\n",
        "                plt.imshow(images[idx])\n",
        "                plt.axis('off')\n",
        "                idx += 1\n",
        "    fig.suptitle(title, fontsize=30)\n",
        "    \n",
        "    # Showing the figure\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "jt0gDAhM5zaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_images_rescale(generated, \"Final result\")"
      ],
      "metadata": {
        "id": "n_0cZ6Ws5119"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gHaH4BYv4gvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Probabilistic PCA"
      ],
      "metadata": {
        "id": "BE0oQUN9RBss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generic PCA Elbow Plot"
      ],
      "metadata": {
        "id": "mIxG_ctjeRf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "KPZw3mKURDvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## from PCA\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "testset = datasets.MNIST(root='./data', train=False, download=False, transform=transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "_cGIVqHLRH7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "### from generic VA\n",
        "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
        "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False)\n",
        "'''"
      ],
      "metadata": {
        "id": "Vqfv9jmmr5-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_matrix(dataloader):\n",
        "    data_matrix = []\n",
        "    for i, (images, labels) in enumerate(dataloader):\n",
        "        images = images.view(-1, 28 * 28)\n",
        "        data_matrix.append(images)\n",
        "    return torch.vstack(data_matrix)\n",
        "\n",
        "train_data_matrix = get_data_matrix(trainloader)\n",
        "test_data_matrix = get_data_matrix(testloader)"
      ],
      "metadata": {
        "id": "pTtUQg03ROR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA()\n",
        "pca.fit(train_data_matrix.numpy())\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n"
      ],
      "metadata": {
        "id": "3q6HfmmYcgD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_elbow_curve(explained_variance_ratio, max_components=200):\n",
        "    cum_explained_variance = np.cumsum(explained_variance_ratio[:max_components])\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(1, max_components + 1), cum_explained_variance)\n",
        "    plt.xlabel('Number of Components')\n",
        "    plt.ylabel('Cumulative Explained Variance')\n",
        "    plt.title('Elbow Plot')\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "plot_elbow_curve(explained_variance_ratio)"
      ],
      "metadata": {
        "id": "4d1wt2HZeciV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_matrix.size()\n"
      ],
      "metadata": {
        "id": "5HakhJ4XRalN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProbabilisticPCA(torch.nn.Module):\n",
        "    def __init__(self, n_components):\n",
        "        super(ProbabilisticPCA, self).__init__()\n",
        "        self.W = torch.nn.Parameter(torch.randn(28 * 28, n_components) * 0.01, requires_grad=True)\n",
        "        self.mu = torch.nn.Parameter(torch.zeros(1, 28 * 28), requires_grad=True)\n",
        "\n",
        "    def forward(self, X):\n",
        "        return (X - self.mu) @ self.W\n",
        "\n",
        "    def reconstruct(self, Z):\n",
        "        return Z @ self.W.t() + self.mu"
      ],
      "metadata": {
        "id": "ggQ4eapIRiG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_probabilistic_pca(train_data, n_components, learning_rate, num_epochs):\n",
        "    model = ProbabilisticPCA(n_components)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        Z = model(train_data)\n",
        "        reconstruction = model.reconstruct(Z)\n",
        "        loss = torch.mean(torch.square(train_data - reconstruction))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "CmtMlhiYZgSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_components = 100\n",
        "learning_rate = 1e-3\n",
        "num_epochs = 250\n",
        "\n",
        "model = train_probabilistic_pca(train_data_matrix, n_components, learning_rate, num_epochs)\n",
        "train_data_reduced = model(train_data_matrix).detach().numpy()"
      ],
      "metadata": {
        "id": "ye1bpHpuZk6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the labels for the training data\n",
        "train_labels = trainset.targets.numpy()\n",
        "\n",
        "test_data_reduced = model(test_data_matrix).detach().numpy()\n",
        "\n",
        "# Get the labels for the test data\n",
        "test_labels = testset.targets.numpy()"
      ],
      "metadata": {
        "id": "RXskatn4ZoZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels.shape\n"
      ],
      "metadata": {
        "id": "aqV3WPaU3F9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## h/t https://stackoverflow.com/questions/36522220/searching-a-sequence-in-a-numpy-array\n",
        "\n",
        "def search_sequence_numpy(arr,seq):\n",
        "    \"\"\" Find sequence in an array using NumPy only.\n",
        "\n",
        "    Parameters\n",
        "    ----------    \n",
        "    arr    : input 1D array\n",
        "    seq    : input 1D array\n",
        "\n",
        "    Output\n",
        "    ------    \n",
        "    Output : 1D Array of indices in the input array that satisfy the \n",
        "    matching of input sequence in the input array.\n",
        "    In case of no match, an empty list is returned.\n",
        "    \"\"\"\n",
        "\n",
        "    # Store sizes of input array and sequence\n",
        "    Na, Nseq = arr.size, seq.size\n",
        "\n",
        "    # Range of sequence\n",
        "    r_seq = np.arange(Nseq)\n",
        "\n",
        "    # Create a 2D array of sliding indices across the entire length of input array.\n",
        "    # Match up with the input sequence & get the matching starting indices.\n",
        "    M = (arr[np.arange(Na-Nseq+1)[:,None] + r_seq] == seq).all(1)\n",
        "\n",
        "    # Get the range of those indices as final output\n",
        "    if M.any() >0:\n",
        "        return np.where(np.convolve(M,np.ones((Nseq),dtype=int))>0)[0]\n",
        "    else:\n",
        "        return []         # No match found"
      ],
      "metadata": {
        "id": "QsQyUUOo4I4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr = test_labels\n",
        "seq = np.array([4,4,9,2,5,4,7,6,7,9,0,5])\n",
        "search_sequence_numpy(arr,seq)"
      ],
      "metadata": {
        "id": "ASg2kWTI4O77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_reduced_data(data, labels, title):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    scatter = plt.scatter(data[:, 0], data[:, 1], c=labels, cmap='tab10', s=10, alpha=0.8)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Component 1')\n",
        "    plt.ylabel('Component 2')\n",
        "    plt.colorbar(scatter)\n",
        "    plt.show()\n",
        "\n",
        "# Plot the reduced training data\n",
        "plot_reduced_data(train_data_reduced, train_labels, 'Probabilistic PCA of MNIST Train Data')"
      ],
      "metadata": {
        "id": "Tc_bc9Zd23nD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the reduced test data\n",
        "plot_reduced_data(test_data_reduced, test_labels, 'Probabilistic PCA of MNIST Test Data')"
      ],
      "metadata": {
        "id": "-GvqAvcGak4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reconstructed_test_data = model.reconstruct(torch.tensor(test_data_reduced)).detach().numpy()\n"
      ],
      "metadata": {
        "id": "D23N2kIHbXgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_reconstructed_images(test_data, reconstructed_data, labels, num_samples=5):\n",
        "    idx = np.random.choice(test_data.shape[0], num_samples, replace=False)\n",
        "    original_images = test_data[idx].reshape(-1, 28, 28)\n",
        "    reconstructed_images = reconstructed_data[idx].reshape(-1, 28, 28)\n",
        "    label_samples = labels[idx]\n",
        "\n",
        "    fig, axes = plt.subplots(nrows=2, ncols=num_samples, figsize=(2*num_samples, 4))\n",
        "\n",
        "    for i, ax in enumerate(axes[0]):\n",
        "        ax.imshow(original_images[i], cmap='gray')\n",
        "        ax.set_title(f'Label: {label_samples[i]}')\n",
        "        ax.axis('off')\n",
        "\n",
        "    for i, ax in enumerate(axes[1]):\n",
        "        ax.imshow(reconstructed_images[i], cmap='gray')\n",
        "        ax.set_title('Reconstructed')\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "HhBgHEDmb51W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_reconstructed_images(test_data_matrix.numpy(), reconstructed_test_data, test_labels)"
      ],
      "metadata": {
        "id": "9p8eWj3AcbR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test data plot"
      ],
      "metadata": {
        "id": "782WaooQUHhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#original_images = test_data[idx].reshape(-1, 28, 28)\n",
        "\n",
        "reconstructed_test_data = model.reconstruct(torch.tensor(test_data_reduced))\n",
        "print(reconstructed_test_data.size())\n",
        "print(\"The size of the test set is: \" + str(reconstructed_test_data.size()))\n",
        "#reconstructed_test_data = reconstructed_test_data[-12:,:]\n",
        "reconstructed_test_data = reconstructed_test_data[116:128:,:]\n",
        "print(reconstructed_test_data.size())"
      ],
      "metadata": {
        "id": "arGA5sgCej39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## now create an image of estimated images and ground truth\n",
        "# h/t https://stackoverflow.com/questions/66667949/pytorch-mnist-autoencoder-to-learn-10-digit-classification\n",
        "\n",
        "## run first five training images through the encoder\n",
        "### from https://github.com/dataflowr/notebooks/blob/master/HW3/VAE_clustering_empty.ipynb\n",
        "\n",
        "def show(img):\n",
        "    npimg = img.cpu().numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
        "    plt.suptitle('Test set reconstruction VAE & ConvNet VAE', fontsize=10, y=.67)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "def plot_reconstruction(vae, n=12):\n",
        "    \n",
        "    x = test_images_ground_truth\n",
        "    x = x[:n,:,:,:].to(device)\n",
        "    print(\"Ground truth images reformatted are of dimension\", x.size())\n",
        "    try:\n",
        "        out, _, _, log_p = vae(x.view(-1, image_size)) \n",
        "    except:\n",
        "        out, _, _ = vae(x.view(-1, image_size))\n",
        "    x_concat = torch.cat([x.view(-1, 1, 28, 28), out.view(-1, 1, 28, 28)], dim=0)\n",
        "    print(\"Concatenated object containing both ground truth & generic VAE is of size\", out.size())\n",
        "    ## Conv Net\n",
        "    out, mu, logVAR = net(x)\n",
        "    x_concat = torch.cat([x_concat, out.view(-1, 1, 28, 28)], dim=0)\n",
        "    ## Prob PCA\n",
        "    x_concat = torch.cat([x_concat, reconstructed_test_data.view(-1, 1, 28, 28).to(device)], dim=0)\n",
        "\n",
        "    \n",
        "    \n",
        "    print(\"Concatenated object also containing ConvNet VAE is of size\", x_concat.size())\n",
        "\n",
        "    print(out.size())\n",
        "    print(out.type())\n",
        "    ##\n",
        "    out_grid = torchvision.utils.make_grid(x_concat, nrow=12)#.cpu().data\n",
        "    show(out_grid)\n",
        "    print(x_concat.size())\n",
        "\n",
        "plot_reconstruction(vae)"
      ],
      "metadata": {
        "id": "Bkwi3Nc0ehvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "hHmMRLJLWFOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m6o6Mf1WpVr4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}