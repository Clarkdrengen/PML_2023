{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Clarkdrengen/PML_2023/blob/main/pytorch-mnist-VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NZkUZwpQaPyJ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Import necessary libraries to create a variational autoencoder\n",
        "The code is mainly developed using the PyTorch library\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8HiWZxh5aPyM",
        "outputId": "c3c59587-e4fa-4afb-a7c0-6793b793d8aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Determine if any GPUs are available \n",
        "\"\"\"\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "tgbqezBzJw6b",
        "outputId": "cc812b78-7477-4102-e1a1-83051d5d045a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# set the current working directory\n",
        "path = '/content/drive/MyDrive/PML_2023/'\n",
        "os.chdir(path)"
      ],
      "metadata": {
        "id": "0PxGbwC1Jpvs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yXIC0Z0naPyN"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Initialize Hyperparameters\n",
        "\"\"\"\n",
        "batch_size = 128\n",
        "learning_rate = 1e-3\n",
        "num_epochs = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nvJRXVPgaPyN"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Create dataloaders to feed data into the neural network\n",
        "Default MNIST dataset is used and standard train/test split is performed\n",
        "\"\"\"\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('data', train=True, download=True,\n",
        "                    transform=transforms.ToTensor()),\n",
        "    batch_size=batch_size, shuffle=True)\n",
        "#    batch_size=1, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('data', train=False, transform=transforms.ToTensor()),\n",
        "    batch_size=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1Wo5cV7vaPyO"
      },
      "outputs": [],
      "source": [
        "#A Convolutional Variational Autoencoder\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, imgChannels=1, featureDim=32*20*20, zDim=256):\n",
        "        super(VAE, self).__init__()\n",
        "        \n",
        "        # Initializing the 2 convolutional layers and 2 fully-connected layers for the encoder\n",
        "        self.encConv1 = nn.Conv2d(imgChannels, 16, 5)\n",
        "        self.encConv2 = nn.Conv2d(16, 32, 5) \n",
        "        self.encFC1 = nn.Linear(featureDim, zDim)\n",
        "        self.encFC2 = nn.Linear(featureDim, zDim)\n",
        "        \n",
        "        # Initializing the fully-connected layer and 2 convolutional layers for decoder\n",
        "        self.decFC1 = nn.Linear(zDim, featureDim)\n",
        "        self.decConv1 = nn.ConvTranspose2d(32, 16, 5)\n",
        "        self.decConv2 = nn.ConvTranspose2d(16, imgChannels, 5)\n",
        "        \n",
        "    def encoder(self, x):\n",
        "        \n",
        "        # Input is fed into 2 convolutional layers sequentially\n",
        "        # The output feature map are fed into 2 fully-connected layers to predict mean (mu) and variance (logVar)\n",
        "        # Mu and logVar are used for generating middle representation z and KL divergence loss\n",
        "        x = F.relu(self.encConv1(x))\n",
        "        x = F.relu(self.encConv2(x))\n",
        "        x = x.view(-1, 32*20*20)\n",
        "        mu = self.encFC1(x)\n",
        "        logVar = self.encFC2(x)\n",
        "        return mu, logVar\n",
        "    \n",
        "    def reparameterize(self, mu, logVar):\n",
        "        \n",
        "        #Reparameterization takes in the input mu and logVar and sample the mu + std * eps\n",
        "        std = torch.exp(logVar/2)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + std * eps\n",
        "    \n",
        "    def decoder(self, z):\n",
        "        \n",
        "        # z is fed back into a fully-connected layers and then into two transpose convolutional layers\n",
        "        # The generated output is the same size of the original input\n",
        "        x = F.relu(self.decFC1(z))\n",
        "        x = x.view(-1, 32, 20, 20)\n",
        "        x = F.relu(self.decConv1(x))\n",
        "        x = torch.sigmoid(self.decConv2(x))\n",
        "        return x\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        # The entire pipeline of the VAE: encoder -> reparameterization -> decoder\n",
        "        # output, mu, and logVar are returned for loss computation\n",
        "        mu, logVar = self.encoder(x)\n",
        "        z = self.reparameterize(mu, logVar)\n",
        "        out = self.decoder(z)\n",
        "        return out, mu, logVar\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tuKXhmnnaPyV"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Initialize the network and the Adam optimizer\n",
        "\"\"\"\n",
        "net = VAE().to(device)\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jY4GpRGMaPyW",
        "outputId": "2e99a3a2-c055-4935-ab15-de3d7bd4b953"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Loss 12463.7763671875\n",
            "Epoch 1: Loss 11587.185546875\n",
            "Epoch 2: Loss 11159.8369140625\n",
            "Epoch 3: Loss 10643.0\n",
            "Epoch 4: Loss 10366.2412109375\n",
            "Epoch 5: Loss 9883.8046875\n",
            "Epoch 6: Loss 9874.072265625\n",
            "Epoch 7: Loss 9862.73046875\n",
            "Epoch 8: Loss 10111.080078125\n",
            "Epoch 9: Loss 10107.943359375\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Training the network for a given number of epochs\n",
        "The loss after every epoch is printed\n",
        "\"\"\"\n",
        "for epoch in range(num_epochs):\n",
        "    for idx, data in enumerate(train_loader, 0):\n",
        "        imgs, _ = data\n",
        "        imgs = imgs.to(device)\n",
        "        \n",
        "        # Feeding a batch of images into the network to obtain the output image, mu, and logVar\n",
        "        out, mu, logVar = net(imgs)\n",
        "        \n",
        "        # The loss is the BCE loss combined with the KL divergence to ensure the distribution is learnt\n",
        "        kl_divergence = -0.5 * torch.sum(1 + logVar - mu.pow(2) - logVar.exp())\n",
        "        loss = F.binary_cross_entropy(out, imgs, size_average=False) + kl_divergence\n",
        "        \n",
        "        # Backpropagation based on the loss\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    print('Epoch {}: Loss {}'.format(epoch, loss))\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.save(net, '20230418_ConvNet_cpu.pt')\n",
        "net = torch.load(path + '20230418_ConvNet_cpu.pt')"
      ],
      "metadata": {
        "id": "oPG81tUoezim"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikYHRcZLe98u",
        "outputId": "98a80f0d-01ed-46b7-9273-a7e6624f8275"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20230415_ConvNet.pt    20230416_ConvNet.pt\t20230418_ConvNet.pt  mnist_data\n",
            "20230415_Diffusion.pt  20230416_VAE.pt\t\tConvNet.pt\n",
            "20230415_VAE.pt        20230418_ConvNet_cpu.pt\tdata\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "_lOoDQnqLzcI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55f92d4d-ee3a-4035-d19f-3fa62507a58b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "ZOa15XL7aPyX",
        "outputId": "66c2eae4-c5e9-4b2d-cad5-72e7908c6071"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAEOCAYAAAApP3VyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAc80lEQVR4nO3df3TU9Z3v8dfk1xAgmRggGdIEDIigonAvQoygxZISsIcF5O4Ktl1wvf4M7EKOq+UexZ97UrXHcqwRzu7tAd1TxKUrcGS7VI0mXFeClxSWUjUFFiUIiUJNJgQYksz3/sF1mkj4TCaZfGYmeT7O+Z7TzGsy8+ErvPvKNzOfcTmO4wgAAMCShGgvAAAADCyUDwAAYBXlAwAAWEX5AAAAVlE+AACAVZQPAABgFeUDAABYRfkAAABWUT4AAIBVSdFewLcFAgGdOHFCaWlpcrlc0V4OMCA5jqPm5mbl5OQoISE+fkZhdgDRFdbccPrIyy+/7IwePdpxu93OtGnTnD179nTr++rq6hxJHBwcMXDU1dX11YjoUk/nhuMwOzg4YuXoztzokysfb7zxhkpLS7V+/XoVFBRo7dq1Ki4uVm1trbKysozfm5aWJkmaoduVpOS+WB6AENrUqg/0m+C/Rxt6MzekjrPjB0pyXWZ28FFWQO8lJHZ5c5vTqg+ct7o1N1yOE/l/jQUFBZo6dapefvllSRcvh+bl5WnFihX6yU9+Yvxen88nj8ejmZp/+QECoE+1Oa2q1HY1NTUpPT3dynP2Zm5IHWaHawHlA+hLhvJRGXizW3Mj4r/MvXDhgmpqalRUVPTnJ0lIUFFRkXbv3n3J/f1+v3w+X6cDwMAS7tyQmB1APIt4+Th16pTa29uVnZ3d6fbs7GzV19dfcv+ysjJ5PJ7gkZeXF+klAYhx4c4NidkBxLOov4x99erVampqCh51dXXRXhKAOMDsAOJXxF9wOnz4cCUmJqqhoaHT7Q0NDfJ6vZfc3+12y+12R3oZAOJIuHNDYnYA8SziVz5SUlI0ZcoUVVRUBG8LBAKqqKhQYWFhpJ8OQD8Q0bnhOJc/APReoP3yRzf1yVttS0tLtXTpUt14442aNm2a1q5dq5aWFt1999198XQA+gHmBjBw9En5uPPOO/XVV19pzZo1qq+v1+TJk7Vz585LXkwGAN9gbgADR5/s89Eb7PMBRF809vnoLWYHEF3hzI2ov9sFAAAMLJQPAABgFeUDAABYRfkAAABWUT4AAIBVlA8AAGAV5QMAAFhF+QAAAFZRPgAAgFWUDwAAYBXlAwAAWEX5AAAAVlE+AACAVZQPAABgFeUDAABYRfkAAABWUT4AAIBVlA8AAGAV5QMAAFhF+QAAAFZRPgAAgFWUDwAAYBXlAwAAWEX5AAAAVlE+AACAVZQPAABgFeUDAABYRfkAAABWUT4AAIBVES8fTz75pFwuV6djwoQJkX4aAP0IcwMYWJL64kGvu+46vfvuu39+kqQ+eRoA/QhzAxg4+uRfd1JSkrxeb188NIB+irkBDBx98pqPQ4cOKScnR2PGjNEPf/hDHTt27LL39fv98vl8nQ4AA084c0NidgDxLOLlo6CgQBs3btTOnTu1bt06HT16VLfccouam5u7vH9ZWZk8Hk/wyMvLi/SSAMS4cOeGxOwA4pnLcRynL5+gsbFRo0eP1osvvqh77rnnktzv98vv9we/9vl8ysvL00zNV5IruS+XBuAy2pxWVWq7mpqalJ6ebv35Q80NidkBxJpw5kafv6IrIyNDV199tQ4fPtxl7na75Xa7+3oZAOJIqLkhMTuAeNbn5ePMmTM6cuSIfvzjH/f1U8UM140TjfnTWzaEfIx/900y5v/28+8a84RW8wWtjE8vfzlbkmrvHWLMQ/F8bP6r9Z0t/2XMA2dazLnhcjziX7+dGy6XMU5ITTV/e+7IkE/RNGm4MT85w7yGG6ccMuY3ZRw15p+0mNdY05BrzAe/mmHMPf/xmTFva/jSmKtvL/ajmyL+mo+HH35YVVVV+uyzz/Thhx9q4cKFSkxM1JIlSyL9VAD6CeYGMLBE/MrH8ePHtWTJEp0+fVojRozQjBkzVF1drREjRkT6qQD0E8wNYGCJePnYvHlzpB8SQD/H3AAGFj7bBQAAWEX5AAAAVlE+AACAVZQPAABgFR8b2QfOfmewMR+X1BryMaYMP2jMH/sHcx5180Lkj5rjFSduNuZHpoa3HCAWJGZkGPO6e68x5vk/MO+PI0n/kLvVmF+bYt4j54qEQcY82ZVoXkCmeY3tuQFj/sXks8b8l18XGPMtvzbvgZS/0fyZQW1fnDTmCrSbc3QLVz4AAIBVlA8AAGAV5QMAAFhF+QAAAFZRPgAAgFWUDwAAYBXlAwAAWEX5AAAAVrHJWB9I3f6RMZ9+w8MhH+MHC3cb82ezzc+RJPNGQKcD54z5jH8OsUaXOX76DvOnlP7l0NPG/GcjdxnzWUv+1pinv15tzIFoaG/yGfNhB80bEP5+XF7I5/iZU2zMG86kGXNfi3mTMSdg/sffdsr8/W6veROxssnmTdL+yrPXmN+27BNjfu81f23Mxz2TasxVf8oYB5rNm7g5bW3mxx8guPIBAACsonwAAACrKB8AAMAqygcAALCK8gEAAKyifAAAAKsoHwAAwCr2+YiCvGc+DHmfA8+Y8+/PLzHmTqL5vfjJZ9qNef7b5n1GQvnnf/quMf9fT2Ya80Oz/rcxbxpj7s3pxhSIkoD5313q//nUmI/7ekzIpziXmmPMs/5Yb8xH+Mx5wO83LyDgGGPXILcxf3HWXca89f4QewSN32LM/2L8AWNec+UUYz74T03G3Anx58dFXPkAAABWUT4AAIBVlA8AAGAV5QMAAFhF+QAAAFZRPgAAgFWUDwAAYBX7fMSp1O0fRXsJRl/dMtKY/9P0f7S0EiB+BJqbjblrz8GQj5GcYN7jp63dvNeInL7dp8KVGOJn3hBPPyipzZifbh9qzLdWTzXm13x8wpi3NXxpzNE9YV/52LVrl+bNm6ecnBy5XC5t27atU+44jtasWaORI0cqNTVVRUVFOnToUKTWCyAOMTcAdBR2+WhpadGkSZNUXl7eZf7888/rpZde0vr167Vnzx4NGTJExcXFOn/+fK8XCyA+MTcAdBT2r13mzp2ruXPndpk5jqO1a9fqscce0/z58yVJr732mrKzs7Vt2zYtXrz4ku/x+/3yd9iu1+fzhbskADEu0nNDYnYA8SyiLzg9evSo6uvrVVRUFLzN4/GooKBAu3d3/VkhZWVl8ng8wSMvLy+SSwIQ43oyNyRmBxDPIlo+6usvfiBRdnZ2p9uzs7OD2betXr1aTU1NwaOuri6SSwIQ43oyNyRmBxDPov5uF7fbLbfb/CmHAPBtzA4gfkX0yofX65UkNTQ0dLq9oaEhmAFAR8wNYOCJ6JWP/Px8eb1eVVRUaPLkyZIuvghsz549evDBByP5VIiy0/+z0Jgvf/hfjfnMQa3G/IHjtxjzK1/5xJiH2MkAMYS5EYZA6L/ZTqCP1+Ay7yOSEOJqVGDy1ca88W7zXicvj91uzJ85Os+YTyj/2pi3HTtuzBEZYZePM2fO6PDhw8Gvjx49qv379yszM1OjRo3SypUr9eyzz2rcuHHKz8/X448/rpycHC1YsCCS6wYQR5gbADoKu3zs3btXt912W/Dr0tJSSdLSpUu1ceNGPfLII2ppadF9992nxsZGzZgxQzt37tSgQYMit2oAcYW5AaCjsMvHzJkz5Ri233W5XHr66af19NNP92phAPoP5gaAjvhgOQAAYBXlAwAAWEX5AAAAVlE+AACAVVHf4RSxKfGaccZ8zaOvGvPiwU3GfMUJ8z4ex79njBVoMb9XH0DPuJJTjHnisCuM+bmJucb86J3mn3mfmLDTmL/tu96Y+18aacwTD+035jK8MBqRw5UPAABgFeUDAABYRfkAAABWUT4AAIBVlA8AAGAV5QMAAFhF+QAAAFaxzwe6VHvvMGP+g8FnjPm4ivvN+V//Luw1Aeg9V5J57CfmZBvz0zO+Y8y//ouzxrzoyiPGPJR/+fcZxvyqDw8b8/bWC716fkQGVz4AAIBVlA8AAGAV5QMAAFhF+QAAAFZRPgAAgFWUDwAAYBXlAwAAWMU+H+jSmElf9Or787bwVwuICpfLGCekpRnzC6OGG/M/XWd+/PYLica8riXDmP+m9XpjPvw/HWMeaGwy5ogNXPkAAABWUT4AAIBVlA8AAGAV5QMAAFhF+QAAAFZRPgAAgFWUDwAAYBWbMaBL/3XS/F5/TTDHQ0qPG3Pnc/MDBA58an4CYCAKsYeHJLmSks13yBpmjJtHu0M8gTl2fCnGvNY/0phfnV9vzM8NM//M7Ekx//md1gvGHHaEfeVj165dmjdvnnJycuRyubRt27ZO+bJly+RyuTodc+bMidR6AcQh5gaAjsIuHy0tLZo0aZLKy8sve585c+bo5MmTweP111/v1SIBxDfmBoCOwv61y9y5czV37lzjfdxut7xeb48XBaB/YW4A6KhPXnBaWVmprKwsjR8/Xg8++KBOnz592fv6/X75fL5OB4CBJ5y5ITE7gHgW8fIxZ84cvfbaa6qoqNBzzz2nqqoqzZ07V+3t7V3ev6ysTB6PJ3jk5eVFekkAYly4c0NidgDxLOLvdlm8eHHwf19//fW64YYbNHbsWFVWVmrWrFmX3H/16tUqLS0Nfu3z+RgiwAAT7tyQmB1APOvzfT7GjBmj4cOH6/Dhw13mbrdb6enpnQ4AA1uouSExO4B41uf7fBw/flynT5/WyJHm93YjtoxfecyY/+z98cb8rat3GPPKreb34r+w+C5j7vzf3xtzxDfmRh8KBIxx2ud+Y+5qN+8DciYv0Zj7/9t5Y16U/Ykx/+JvzPuAvOeeZsxzymuMueM3//kRGWGXjzNnznT6aeTo0aPav3+/MjMzlZmZqaeeekqLFi2S1+vVkSNH9Mgjj+iqq65ScXFxRBcOIH4wNwB0FHb52Lt3r2677bbg19/8znXp0qVat26dDhw4oFdffVWNjY3KycnR7Nmz9cwzz8jtDrFrHoB+i7kBoKOwy8fMmTPlOM5l89/+9re9WhCA/oe5AaAjPlgOAABYRfkAAABWUT4AAIBVlA8AAGBVn+/zgfjUfsr8uRpVi/+7Md+59lpj/u61W83P//pmY/7iksXGnH1A0C8ZXrT75/uY9/FwnTPvY5Fc32TMk4aPMOYpTeY1nmkyv4PpNycnGvO78/7DmH//oYPGfPmEHxvzCSv/YMwDZ88ac3QPVz4AAIBVlA8AAGAV5QMAAFhF+QAAAFZRPgAAgFWUDwAAYBXlAwAAWMU+H+iR9j/UGnPXCzca86v+8n5j/untrxjzwOv/YszXLvkrY84+IOivnLY2Y95e32DMXSkpxjw9xD4h6SnJxjy7wmXMnaFDjPkLs83/th9Y9pYx//X3y435nc/8rTEf+/AeY96tvVjAlQ8AAGAX5QMAAFhF+QAAAFZRPgAAgFWUDwAAYBXlAwAAWEX5AAAAVrHPB/pE8tt7jfnVb5u/f9ai5cb8mkcOGvO/37zJmL/wPxYbc2ffH4w5EK9C7QMSKg+cPdu7BbjM+3yEkvup25j/+sAcY/7lcx8a8zm37jPmh0Psg+L4zfug4CKufAAAAKsoHwAAwCrKBwAAsIryAQAArKJ8AAAAqygfAADAKsoHAACwin0+EJOG/OseY/7Fx+OM+cx3W4354+PTjHma+a3+AHrKcXr17YHz5435oMrfG/M3tn3XmN8y5z+NecLY7xjz9o//aMxxUVhXPsrKyjR16lSlpaUpKytLCxYsUG1tbaf7nD9/XiUlJRo2bJiGDh2qRYsWqaGhIaKLBhBfmB0AOgqrfFRVVamkpETV1dV655131NraqtmzZ6ulpSV4n1WrVumtt97Sli1bVFVVpRMnTuiOO+6I+MIBxA9mB4COwvq1y86dOzt9vXHjRmVlZammpka33nqrmpqa9Mtf/lKbNm3S9773PUnShg0bdM0116i6ulo33XRT5FYOIG4wOwB01KsXnDY1NUmSMjMzJUk1NTVqbW1VUVFR8D4TJkzQqFGjtHv37i4fw+/3y+fzdToA9G/MDmBg63H5CAQCWrlypaZPn66JEydKkurr65WSkqKMjIxO983OzlZ9fX2Xj1NWViaPxxM88vLyerokAHGA2QGgx+WjpKREBw8e1ObNm3u1gNWrV6upqSl41NXV9erxAMQ2ZgeAHr3Vdvny5dqxY4d27dql3Nzc4O1er1cXLlxQY2Njp59gGhoa5PV6u3wst9stt9v8EckA+gdmBwApzPLhOI5WrFihrVu3qrKyUvn5+Z3yKVOmKDk5WRUVFVq0aJEkqba2VseOHVNhYWHkVg0grjA7YEvA7zfmY944ZczfzbvGmF+V2WbME1wuY97bfU76i7DKR0lJiTZt2qTt27crLS0t+LtYj8ej1NRUeTwe3XPPPSotLVVmZqbS09O1YsUKFRYW8mp1YABjdgDoKKzysW7dOknSzJkzO92+YcMGLVu2TJL085//XAkJCVq0aJH8fr+Ki4v1yiuvRGSxAOITswNAR2H/2iWUQYMGqby8XOXl5T1eFID+hdkBoCM+WA4AAFhF+QAAAFZRPgAAgFWUDwAAYBXlAwAAWNWjHU6BvpZwwwRj/snfDbW0EgDxxJWUbMxPTxlmzLO9Xxnz+puyjHnuXvOuu4Hz5435QMGVDwAAYBXlAwAAWEX5AAAAVlE+AACAVZQPAABgFeUDAABYRfkAAABWsc8HosI15Tpj3vis+b3wh2/4x0guBxg4XC5znJhozlNSIrmaSx/fbd4nQyMyjfGpQvM+HFfe+0djXtecYcw9n7Ub85Dnx+8355LUjU+Bjndc+QAAAFZRPgAAgFWUDwAAYBXlAwAAWEX5AAAAVlE+AACAVZQPAABgFft8oEcSr73amH/y92nG/Ncz1xnzySm9+6v5+wutxnzo5+d69fhAf+VKTTXneSONeXv6IGPePHpwiNz8M3HS9D8Z87vGvm3Mrx30hTH/u8+XGPMrT5lni9PWZswHwh4e3cGVDwAAYBXlAwAAWEX5AAAAVlE+AACAVZQPAABgFeUDAABYRfkAAABWhbWZQllZmd588019+umnSk1N1c0336znnntO48ePD95n5syZqqqq6vR9999/v9avXx+ZFcOKY0/ebMx/+zfPG/NRSUONebvTu308Pms7a8xXPbDSmKfs3tur50d4mB3xw+VOMeatw4YY868nmPf5OD3NvA9Gpte8j0dx7qfGfJDL/PhvfDXNmI/Y4TbmKX84Yszbz/uNOS4K68pHVVWVSkpKVF1drXfeeUetra2aPXu2WlpaOt3v3nvv1cmTJ4PH88+b/48KQP/G7ADQUVg/fu7cubPT1xs3blRWVpZqamp06623Bm8fPHiwvF5vZFYIIO4xOwB01KvXfDQ1NUmSMjMzO93+q1/9SsOHD9fEiRO1evVqnT17+Uvkfr9fPp+v0wGgf2N2AANbj3/xHggEtHLlSk2fPl0TJ04M3n7XXXdp9OjRysnJ0YEDB/Too4+qtrZWb775ZpePU1ZWpqeeeqqnywAQZ5gdAHpcPkpKSnTw4EF98MEHnW6/7777gv/7+uuv18iRIzVr1iwdOXJEY8eOveRxVq9erdLS0uDXPp9PeXl5PV0WgBjH7ADQo/KxfPly7dixQ7t27VJubq7xvgUFBZKkw4cPdzlA3G633G7zq4sB9A/MDgBSmOXDcRytWLFCW7duVWVlpfLz80N+z/79+yVJI0eaP4YZQP/F7ADQkctxHKe7d37ooYe0adMmbd++vdP78z0ej1JTU3XkyBFt2rRJt99+u4YNG6YDBw5o1apVys3NveT9+5fj8/nk8Xg0U/OV5EoO/08EoNfanFZVaruampqUnp7e68djdvQjLpc5TjHvE5KQat4HRK7e7X3ptJn3+XAuXDDnfvbp6Klw5kZY5cN1mb90GzZs0LJly1RXV6cf/ehHOnjwoFpaWpSXl6eFCxfqscce6/YAY4AA0Rfp8sHs6EcoH7iMcOZG2L92McnLy+v2TykABg5mB4CO+GwXAABgFeUDAABYRfkAAABWUT4AAIBVlA8AAGBVj7dXB4CY5HJd/u2g3d9ZAJcT4hyGeqtqO29lhbjyAQAALKN8AAAAqygfAADAKsoHAACwivIBAACsonwAAACrYu6ttt98AFWbWiXeFQdERZtaJYX+QLhYEpwdTqvpTpZWAww84cyNmCsfzc3NkqQP9JsorwRAc3OzPB5PtJfRLX+eHf/GDy5AFHVnbricGPvRJhAI6MSJE0pLS5PL5ZLP51NeXp7q6uqUnp4e7eXFJc5h7wzE8+c4jpqbm5WTk6OEhPj47SyzI7I4f7030M5hOHMj5q58JCQkKDc395Lb09PTB8R/vL7EOeydgXb+4uWKxzeYHX2D89d7A+kcdnduxMePNAAAoN+gfAAAAKtivny43W498cQTcrvd0V5K3OIc9g7nLz7x3613OH+9xzm8vJh7wSkAAOjfYv7KBwAA6F8oHwAAwCrKBwAAsIryAQAArKJ8AAAAq2K+fJSXl+vKK6/UoEGDVFBQoI8++ijaS4pZu3bt0rx585STkyOXy6Vt27Z1yh3H0Zo1azRy5EilpqaqqKhIhw4dis5iY1BZWZmmTp2qtLQ0ZWVlacGCBaqtre10n/Pnz6ukpETDhg3T0KFDtWjRIjU0NERpxbgc5kb3MTd6h7nRMzFdPt544w2VlpbqiSee0O9+9ztNmjRJxcXF+vLLL6O9tJjU0tKiSZMmqby8vMv8+eef10svvaT169drz549GjJkiIqLi3X+/HnLK41NVVVVKikpUXV1td555x21trZq9uzZamlpCd5n1apVeuutt7RlyxZVVVXpxIkTuuOOO6K4anwbcyM8zI3eYW70kBPDpk2b5pSUlAS/bm9vd3JycpyysrIorio+SHK2bt0a/DoQCDher9d54YUXgrc1NjY6brfbef3116Owwtj35ZdfOpKcqqoqx3Eunq/k5GRny5Ytwft88sknjiRn9+7d0VomvoW50XPMjd5jbnRPzF75uHDhgmpqalRUVBS8LSEhQUVFRdq9e3cUVxafjh49qvr6+k7n0+PxqKCggPN5GU1NTZKkzMxMSVJNTY1aW1s7ncMJEyZo1KhRnMMYwdyILOZG+Jgb3ROz5ePUqVNqb29XdnZ2p9uzs7NVX18fpVXFr2/OGeezewKBgFauXKnp06dr4sSJki6ew5SUFGVkZHS6L+cwdjA3Iou5ER7mRvclRXsBQCwqKSnRwYMH9cEHH0R7KQDiBHOj+2L2ysfw4cOVmJh4ySuCGxoa5PV6o7Sq+PXNOeN8hrZ8+XLt2LFD77//vnJzc4O3e71eXbhwQY2NjZ3uzzmMHcyNyGJudB9zIzwxWz5SUlI0ZcoUVVRUBG8LBAKqqKhQYWFhFFcWn/Lz8+X1ejudT5/Ppz179nA+/z/HcbR8+XJt3bpV7733nvLz8zvlU6ZMUXJycqdzWFtbq2PHjnEOYwRzI7KYG6ExN3oo2q94Ndm8ebPjdrudjRs3Oh9//LFz3333ORkZGU59fX20lxaTmpubnX379jn79u1zJDkvvviis2/fPufzzz93HMdxfvrTnzoZGRnO9u3bnQMHDjjz58938vPznXPnzkV55bHhwQcfdDwej1NZWemcPHkyeJw9ezZ4nwceeMAZNWqU89577zl79+51CgsLncLCwiiuGt/G3AgPc6N3mBs9E9Plw3Ec5xe/+IUzatQoJyUlxZk2bZpTXV0d7SXFrPfff9+RdMmxdOlSx3Euvm3u8ccfd7Kzsx232+3MmjXLqa2tje6iY0hX506Ss2HDhuB9zp075zz00EPOFVdc4QwePNhZuHChc/LkyegtGl1ibnQfc6N3mBs943Icx7F3nQUAAAx0MfuaDwAA0D9RPgAAgFWUDwAAYBXlAwAAWEX5AAAAVlE+AACAVZQPAABgFeUDAABYRfkAAABWUT4AAIBVlA8AAGDV/wM0cajC0XUZWgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "\"\"\"\n",
        "The following part takes a random image from test loader to feed into the VAE.\n",
        "Both the original image and generated image from the distribution are shown.\n",
        "\"\"\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "net.eval()\n",
        "with torch.no_grad():\n",
        "    for data in random.sample(list(test_loader), 1):\n",
        "        imgs, _ = data\n",
        "        imgs = imgs.to(device)\n",
        "        img = np.transpose(imgs[0].cpu().numpy(), [1,2,0])\n",
        "        plt.subplot(121)\n",
        "        plt.imshow(np.squeeze(img))\n",
        "        out, mu, logVAR = net(imgs)\n",
        "        outimg = np.transpose(out[0].cpu().numpy(), [1,2,0])\n",
        "        plt.subplot(122)\n",
        "        plt.imshow(np.squeeze(outimg))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate RMSE and MLP classification accuracy"
      ],
      "metadata": {
        "id": "YBGtxjqtP0XK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consolidate train images + labels"
      ],
      "metadata": {
        "id": "lxjc_b9CvMWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "reload training set with batch size = 1\n",
        "\"\"\"\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('data', train=True, download=True,\n",
        "                    transform=transforms.ToTensor()),\n",
        "#    batch_size=batch_size, shuffle=True)\n",
        "    batch_size=1, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('data', train=False, transform=transforms.ToTensor()),\n",
        "    batch_size=1)"
      ],
      "metadata": {
        "id": "KelnTQPjX8El"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = []\n",
        "for batch in train_loader:\n",
        "    images.append(batch[0])\n",
        "train_images = torch.cat(images, dim=0)\n",
        "\n",
        "train_labels = []\n",
        "for _, label in train_loader:\n",
        "    train_labels.append(label)"
      ],
      "metadata": {
        "id": "griNtIhvvOOP"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_labels)"
      ],
      "metadata": {
        "id": "xf_2wLHb6OLF",
        "outputId": "cd1066a6-df42-498f-f657-c16bef918b7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = torch.as_tensor(train_labels, dtype=torch.float)\n",
        "print(train_labels.dtype)\n",
        "print(len(list(train_labels)))"
      ],
      "metadata": {
        "id": "VyRYejAd3PMP",
        "outputId": "27262b20-e49e-42e9-9df4-e94b8133888f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float32\n",
            "60000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consolidate test images + labels"
      ],
      "metadata": {
        "id": "qYatNT7-QAd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images = []\n",
        "for batch in test_loader:\n",
        "    images.append(batch[0])\n",
        "test_images = torch.cat(images, dim=0)\n",
        "\n",
        "test_labels = []\n",
        "for _, label in test_loader:\n",
        "    test_labels.append(label)\n",
        "test_labels = torch.as_tensor(test_labels, dtype=torch.float)\n",
        "test_labels = test_labels.type(torch.LongTensor)   # casting to long\n",
        "print(len(test_labels))\n",
        "print(test_labels.size())\n",
        "print(test_labels.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiCNXKf6QahB",
        "outputId": "abfd826d-45cf-4f94-f4e2-54ec5f74274a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n",
            "torch.Size([10000])\n",
            "torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consolidate synthetic images"
      ],
      "metadata": {
        "id": "4Zjz9CRaW29w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an empty tensor to store the synthetic images\n",
        "synthetic_images = torch.empty((len(test_loader), 1, 28, 28), dtype=torch.float32, device=device)\n",
        "\n",
        "# Evaluate the Conv Net VAE model on the test dataset and generate synthetic images\n",
        "net.eval()\n",
        "with torch.no_grad():\n",
        "    for i, (image, label) in enumerate(test_loader):\n",
        "        image = image.to(device)\n",
        "        synthetic_image = net(image)[0].squeeze()\n",
        "        synthetic_images[i] = synthetic_image\n"
      ],
      "metadata": {
        "id": "oh2O3785W4OC"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# THE BIG QUESTINO IS WHETHER THE LABELS SHOULD BE IN A TENSOR OR NOT. IF NOT? WHAT IS THE ALTERNATIVE? HOW CAN I FIND OUT?\n",
        "test_labels.size()"
      ],
      "metadata": {
        "id": "ryPqCesn81Qm",
        "outputId": "6d57a7a7-d4d2-45da-e946-2a52311c3ee8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10000])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the updated dataset (assuming it's a list of (image, label) tuples)\n",
        "synthetic_data = []\n",
        "for i in range(synthetic_images.size()[0]):\n",
        "  synthetic_data.append((synthetic_image, test_labels))"
      ],
      "metadata": {
        "id": "NjQdIiBO2wAd"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a custom dataset class\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n"
      ],
      "metadata": {
        "id": "O16rKJpg5gZs"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create customer dataloader\n",
        "custom_dataset = CustomDataset(synthetic_data)\n",
        "synthetic_dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "kRoJ_l_r_BU9"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create ad hoc synthetic data loader"
      ],
      "metadata": {
        "id": "kztz9T2cQAp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Instantiate the custom dataset with the updated data\n",
        "custom_dataset = CustomDataset(synthetic_data)\n",
        "\n",
        "# Create a new DataLoader with the updated dataset\n",
        "synthetic_dataloader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "WXsK7fge-H44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# deprecated\n",
        "\n",
        "# h/t https://machinelearningmastery.com/training-a-pytorch-model-with-dataloader-and-dataset/\n",
        "# set up DataLoader for training set\n",
        "synthetic_loader = DataLoader(list(zip(synthetic_images, test_labels)), shuffle=False, batch_size=100)\n",
        "\n",
        "print(len(list(synthetic_loader)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgD3fmYsQcDx",
        "outputId": "4a967a7a-fc11-4de7-e6d8-5b4f2b762b0e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare layout in synthetic_loader vs. synthetic_dataloader"
      ],
      "metadata": {
        "id": "daKeClKSA-l7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = []\n",
        "b = []\n",
        "for image, label in train_loader:\n",
        "    a.append(image)\n",
        "    b.append(label)"
      ],
      "metadata": {
        "id": "tOAY6QFdA916"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(a[0].shape)\n",
        "print(b[0].shape)"
      ],
      "metadata": {
        "id": "kDWt7lHHA95B",
        "outputId": "8dd2ebb7-67f5-4fb9-b460-8363762f4040",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 28, 28])\n",
            "torch.Size([128, 10000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = []\n",
        "b = []\n",
        "for image, label in synthetic_loader:\n",
        "    a.append(image)\n",
        "    b.append(label)\n",
        "\n",
        "print(a[0].shape)\n",
        "print(b[0].shape)"
      ],
      "metadata": {
        "id": "e0ntAwBABw9o",
        "outputId": "88db51ac-1aca-4bff-9e13-6e8fe60f2089",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100, 1, 28, 28])\n",
            "torch.Size([100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = []\n",
        "b = []\n",
        "for image, label in synthetic_dataloader:\n",
        "    a.append(image)\n",
        "    b.append(label)\n",
        "\n",
        "print(a[0].shape)\n",
        "print(b[0].shape)"
      ],
      "metadata": {
        "id": "frHUi0JvBxAU",
        "outputId": "7dd2e0bb-4901-442a-aaf2-ed60b086572c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 28, 28])\n",
            "torch.Size([128, 10000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save synthetic loader"
      ],
      "metadata": {
        "id": "N2ZBaVmjMFU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "ls6IyNFJMTV1",
        "outputId": "825aaa03-4bb0-4c29-8781-3c7daacf0ca7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20230415_ConvNet.pt    20230416_VAE.pt\t\tdata\n",
            "20230415_Diffusion.pt  20230418_ConvNet_cpu.pt\tmnist_data\n",
            "20230415_VAE.pt        20230418_ConvNet.pt\tsynthetic_dataloader.pth\n",
            "20230416_ConvNet.pt    ConvNet.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(synthetic_dataloader, \"synthetic_dataloader.pth\")\n",
        "torch.save(synthetic_loader, \"synthetic_loader.pth\")"
      ],
      "metadata": {
        "id": "G9ScwLtoMELt"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create MLP"
      ],
      "metadata": {
        "id": "EKuW9nF1QAsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(synthetic_dataloader.__class__)"
      ],
      "metadata": {
        "id": "fB3VZ7lHRXW9",
        "outputId": "2fc3cc8c-3a16-4632-97d2-0b2391b55cf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.utils.data.dataloader.DataLoader'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "## Define the NN architecture\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 256)\n",
        "        # linear layer (n_hidden -> hidden_2)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        # linear layer (n_hidden -> 10)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # flatten image input\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        # add hidden layer, with relu activation function\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "# initialize the NN\n",
        "model = Net()\n",
        "print(model)"
      ],
      "metadata": {
        "id": "3xeMicbYNdBL",
        "outputId": "5afdf5ef-2f84-4351-b462-3763be18b4fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=784, out_features=256, bias=True)\n",
            "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (fc3): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Specify loss and optimization functions\n",
        "\n",
        "# specify loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# specify optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "YeBwnwLINdDw"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of epochs to train the model\n",
        "n_epochs = 50  # suggest training between 20-50 epochs\n",
        "\n",
        "model.train() # prep model for training\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    # monitor training loss\n",
        "    train_loss = 0.0\n",
        "    \n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "    for data, target in synthetic_loader:\n",
        "    #for data, target in synthetic_dataloader:\n",
        "\n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # calculate the loss\n",
        "        loss = criterion(output, target)\n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        # update running training loss\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "        \n",
        "    # print training statistics \n",
        "    # calculate average loss over an epoch\n",
        "    train_loss = train_loss/len(synthetic_loader.dataset)\n",
        "\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
        "        epoch+1, \n",
        "        train_loss\n",
        "        ))"
      ],
      "metadata": {
        "id": "GPaHqUoeNdGp",
        "outputId": "d91793f4-d302-4116-9939-378957fe4f53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 \tTraining Loss: 2.287025\n",
            "Epoch: 2 \tTraining Loss: 2.256120\n",
            "Epoch: 3 \tTraining Loss: 2.210263\n",
            "Epoch: 4 \tTraining Loss: 2.136869\n",
            "Epoch: 5 \tTraining Loss: 2.014781\n",
            "Epoch: 6 \tTraining Loss: 1.819890\n",
            "Epoch: 7 \tTraining Loss: 1.559933\n",
            "Epoch: 8 \tTraining Loss: 1.306294\n",
            "Epoch: 9 \tTraining Loss: 1.121623\n",
            "Epoch: 10 \tTraining Loss: 1.000077\n",
            "Epoch: 11 \tTraining Loss: 0.860234\n",
            "Epoch: 12 \tTraining Loss: 0.732266\n",
            "Epoch: 13 \tTraining Loss: 0.655360\n",
            "Epoch: 14 \tTraining Loss: 0.600823\n",
            "Epoch: 15 \tTraining Loss: 0.560090\n",
            "Epoch: 16 \tTraining Loss: 0.528496\n",
            "Epoch: 17 \tTraining Loss: 0.503200\n",
            "Epoch: 18 \tTraining Loss: 0.482433\n",
            "Epoch: 19 \tTraining Loss: 0.465020\n",
            "Epoch: 20 \tTraining Loss: 0.450131\n",
            "Epoch: 21 \tTraining Loss: 0.437195\n",
            "Epoch: 22 \tTraining Loss: 0.425800\n",
            "Epoch: 23 \tTraining Loss: 0.415676\n",
            "Epoch: 24 \tTraining Loss: 0.406576\n",
            "Epoch: 25 \tTraining Loss: 0.398348\n",
            "Epoch: 26 \tTraining Loss: 0.390855\n",
            "Epoch: 27 \tTraining Loss: 0.383978\n",
            "Epoch: 28 \tTraining Loss: 0.377634\n",
            "Epoch: 29 \tTraining Loss: 0.371755\n",
            "Epoch: 30 \tTraining Loss: 0.366279\n",
            "Epoch: 31 \tTraining Loss: 0.361201\n",
            "Epoch: 32 \tTraining Loss: 0.356396\n",
            "Epoch: 33 \tTraining Loss: 0.351855\n",
            "Epoch: 34 \tTraining Loss: 0.347554\n",
            "Epoch: 35 \tTraining Loss: 0.343474\n",
            "Epoch: 36 \tTraining Loss: 0.339584\n",
            "Epoch: 37 \tTraining Loss: 0.335863\n",
            "Epoch: 38 \tTraining Loss: 0.332289\n",
            "Epoch: 39 \tTraining Loss: 0.328853\n",
            "Epoch: 40 \tTraining Loss: 0.325543\n",
            "Epoch: 41 \tTraining Loss: 0.322355\n",
            "Epoch: 42 \tTraining Loss: 0.319282\n",
            "Epoch: 43 \tTraining Loss: 0.316308\n",
            "Epoch: 44 \tTraining Loss: 0.313426\n",
            "Epoch: 45 \tTraining Loss: 0.310634\n",
            "Epoch: 46 \tTraining Loss: 0.307931\n",
            "Epoch: 47 \tTraining Loss: 0.305309\n",
            "Epoch: 48 \tTraining Loss: 0.302749\n",
            "Epoch: 49 \tTraining Loss: 0.300260\n",
            "Epoch: 50 \tTraining Loss: 0.297835\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## ORIGINAL VEERSION\n",
        "# initialize lists to monitor test loss and accuracy\n",
        "test_loss = 0.0\n",
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "\n",
        "model.eval() # prep model for *evaluation*\n",
        "\n",
        "#for data, target in test_loader:\n",
        "for data, target in train_loader:\n",
        "    # forward pass: compute predicted outputs by passing inputs to the model\n",
        "    output = model(data)\n",
        "    # calculate the loss\n",
        "    loss = criterion(output, target)\n",
        "    # update test loss \n",
        "    test_loss += loss.item()*data.size(0)\n",
        "    # convert output probabilities to predicted class\n",
        "    _, pred = torch.max(output, 1)\n",
        "    # compare predictions to true label\n",
        "    correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
        "    # calculate test accuracy for each object class\n",
        "    for i in range(batch_size):\n",
        "        label = target.data[i]\n",
        "        class_correct[label] += correct[i].item()\n",
        "        #class_correct[label] += correct.item()\n",
        "        class_total[label] += 1\n",
        "\n",
        "# calculate and print avg test loss\n",
        "test_loss = test_loss/len(test_loader.dataset)\n",
        "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "for i in range(10):\n",
        "    if class_total[i] > 0:\n",
        "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "            str(i), 100 * class_correct[i] / class_total[i],\n",
        "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "    else:\n",
        "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "\n",
        "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "    100. * np.sum(class_correct) / np.sum(class_total),\n",
        "    np.sum(class_correct), np.sum(class_total)))\n"
      ],
      "metadata": {
        "id": "0pFsr2KMNdJL",
        "outputId": "467cf877-c096-4b51-d432-6cd0e3bfa647",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-42029083e37c>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mclass_correct\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;31m#class_correct[label] += correct.item()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mclass_total\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_correct = list(0. for i in range(10))\n",
        "print(class_correct)"
      ],
      "metadata": {
        "id": "ldihMlLRnGH-",
        "outputId": "63287670-efb2-46c9-d7e8-72dbc95c579a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize lists to monitor test loss and accuracy\n",
        "test_loss = 0.0\n",
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "\n",
        "model.eval() # prep model for *evaluation*\n",
        "\n",
        "# Iterate through the evaluation dataset\n",
        "for data, target in train_loader:\n",
        "    # forward pass: compute predicted outputs by passing inputs to the model\n",
        "    output = model(data)\n",
        "    # calculate the loss\n",
        "    loss = criterion(output, target)\n",
        "    # update test loss \n",
        "    test_loss += loss.item()*data.size(0)\n",
        "    # convert output probabilities to predicted class\n",
        "    _, pred = torch.max(output, 1)\n",
        "    # compare predictions to true label\n",
        "    correct = torch.squeeze(pred.eq(target.data.view_as(pred)))  # Use torch.squeeze() instead of np.squeeze()\n",
        "    # calculate test accuracy for each object class\n",
        "    for i in range(batch_size):\n",
        "        label = target.data[i].item()  # Use .item() to get the value from the 0-dim tensor\n",
        "        class_correct[label] += correct[i].item()  # This line should work now\n",
        "        class_total[label] += 1\n",
        "\n",
        "\n",
        "# calculate and print avg test loss\n",
        "test_loss = test_loss/len(train_loader.dataset)\n",
        "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "for i in range(10):\n",
        "    if class_total[i] > 0:\n",
        "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "            str(i), 100 * class_correct[i] / class_total[i],\n",
        "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "    else:\n",
        "        print('Test Accuracy of %5s: N/A (no training examples)' % (i))\n",
        "\n",
        "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "    100. * np.sum(class_correct) / np.sum(class_total),\n",
        "    np.sum(class_correct), np.sum(class_total)))\n"
      ],
      "metadata": {
        "id": "YWSAQ4khNdLS",
        "outputId": "95b9677b-b82d-4067-e3ca-03c959e01289",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-8e0ad48a5ea1>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Use .item() to get the value from the 0-dim tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mclass_correct\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# This line should work now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mclass_total\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cOzIIMhbNdNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tZ60BZ1XNdPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7ldfkcYoNdRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ivhHXOYYNdTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train model"
      ],
      "metadata": {
        "id": "8wB-0RYWljRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the MLP model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Create an instance of the MLP model and move it to the selected device\n",
        "model = MLP().to(device)"
      ],
      "metadata": {
        "id": "aw9wANjhlc4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss() # applies a softmax funtion to the output layer and then calculates the log loss.\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "n_epochs = 10\n",
        "model.train()\n",
        "for epoch in range(n_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "      labels = labels.type(torch.LongTensor)   # casting to long\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(images)\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      if (i+1) % 5000 == 0:\n",
        "        print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                  .format(epoch+1, n_epochs, i+1, len(train_loader), loss.item()))\n"
      ],
      "metadata": {
        "id": "SOu0senNllkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate artificial model accuracy"
      ],
      "metadata": {
        "id": "0eUYrh94QUsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.__class__"
      ],
      "metadata": {
        "id": "U5PkEfAty-jB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize lists to monitor test loss and accuracy\n",
        "test_loss = 0.0\n",
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "model = model.to(device)\n",
        "\n",
        "model.eval() # prep model for *evaluation*\n",
        "\n",
        "for data, target in train_loader:\n",
        "    # forward pass: compute predicted outputs by passing inputs to the model\n",
        "    output = model(data)\n",
        "    # calculate the loss\n",
        "    loss = criterion(output, target)\n",
        "    # update test loss \n",
        "    test_loss += loss.item()*data.size(0)\n",
        "    # convert output probabilities to predicted class\n",
        "    _, pred = torch.max(output, 1)\n",
        "    # compare predictions to true label\n",
        "    correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
        "    # calculate test accuracy for each object class\n",
        "    for i in range(batch_size):\n",
        "        label = target.data[i]\n",
        "        class_correct[label] += correct[i].item()\n",
        "        class_total[label] += 1\n",
        "\n",
        "# calculate and print avg test loss\n",
        "test_loss = test_loss/len(test_loader.dataset)\n",
        "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "for i in range(10):\n",
        "    if class_total[i] > 0:\n",
        "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "            str(i), 100 * class_correct[i] / class_total[i],\n",
        "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "    else:\n",
        "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "\n",
        "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "    100. * np.sum(class_correct) / np.sum(class_total),\n",
        "    np.sum(class_correct), np.sum(class_total)))"
      ],
      "metadata": {
        "id": "a6ugfeBNx1km"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# evaluate accuracy after training\n",
        "model.eval()\n",
        "test_tensor = torch.tensor(train_images, dtype=torch.float32)\n",
        "test_tensor.size()\n",
        "\n",
        "#y_pred = model(test_tensor).squeeze().round()\n",
        "#acc = (y_pred == torch.tensor(train_labels, dtype=torch.float32)).float().mean().item()\n",
        "#print(\"Model accuracy: %.2f%%\" % (acc*100))"
      ],
      "metadata": {
        "id": "mImfwzCiQdvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load generic VAE for comparison"
      ],
      "metadata": {
        "id": "nTe-8S4gk3tM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDSaRpXDaPyY"
      },
      "outputs": [],
      "source": [
        "# prerequisites\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper-parameters\n",
        "image_size = 784\n",
        "bs = 128\n",
        "#h_dim = 400\n",
        "#z_dim = 20\n",
        "num_epochs = 50\n",
        "x_dim=784, \n",
        "h_dim1= 512, \n",
        "h_dim2=256, \n",
        "z_dim=2\n",
        "learning_rate = 1e-3"
      ],
      "metadata": {
        "id": "LaoGCObKk97H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MNIST Dataset\n",
        "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
        "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False)"
      ],
      "metadata": {
        "id": "2W496PmFMCbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim1, h_dim2, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        \n",
        "        # encoder part\n",
        "        self.fc1 = nn.Linear(x_dim, h_dim1)\n",
        "        self.fc2 = nn.Linear(h_dim1, h_dim2)\n",
        "        self.fc31 = nn.Linear(h_dim2, z_dim)\n",
        "        self.fc32 = nn.Linear(h_dim2, z_dim)\n",
        "        # decoder part\n",
        "        self.fc4 = nn.Linear(z_dim, h_dim2)\n",
        "        self.fc5 = nn.Linear(h_dim2, h_dim1)\n",
        "        self.fc6 = nn.Linear(h_dim1, x_dim)\n",
        "        \n",
        "    def encoder(self, x):\n",
        "        h = F.relu(self.fc1(x))\n",
        "        h = F.relu(self.fc2(h))\n",
        "        return self.fc31(h), self.fc32(h) # mu, log_var\n",
        "    \n",
        "    def sampling(self, mu, log_var):\n",
        "        std = torch.exp(0.5*log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        return eps.mul(std).add_(mu) # return z sample\n",
        "        \n",
        "    def decoder(self, z):\n",
        "        h = F.relu(self.fc4(z))\n",
        "        h = F.relu(self.fc5(h))\n",
        "        return F.sigmoid(self.fc6(h)) \n",
        "    \n",
        "    def forward(self, x):\n",
        "        mu, log_var = self.encoder(x.view(-1, 784))\n",
        "        z = self.sampling(mu, log_var)\n",
        "        return self.decoder(z), mu, log_var\n",
        "\n",
        "# build model\n",
        "#vae = VAE(x_dim=image_size , h_dim1= 512, h_dim2=256, z_dim=2)\n",
        "vae = VAE(x_dim=image_size , h_dim1= 512, h_dim2=256, z_dim=z_dim)\n",
        "if torch.cuda.is_available():\n",
        "    vae.cuda()"
      ],
      "metadata": {
        "id": "Nga64sE1lAjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vae.parameters"
      ],
      "metadata": {
        "id": "RCBc46hvlH6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(vae.parameters(), lr=learning_rate)\n",
        "# return reconstruction error + KL divergence losses\n",
        "def loss_function(recon_x, x, mu, log_var):\n",
        "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "    return BCE + KLD"
      ],
      "metadata": {
        "id": "XxbM7XXDlIeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    vae.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        if torch.cuda.is_available():\n",
        "          data = data.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        recon_batch, mu, log_var = vae(data)\n",
        "        loss = loss_function(recon_batch, data, mu, log_var)\n",
        "        \n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch_idx % 100 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item() / len(data)))\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))"
      ],
      "metadata": {
        "id": "ZiXHdpXHlMxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "    vae.eval()\n",
        "    test_loss= 0\n",
        "    with torch.no_grad():\n",
        "        for data, _ in test_loader:\n",
        "            if torch.cuda.is_available():\n",
        "              data = data.cuda()\n",
        "            recon, mu, log_var = vae(data)\n",
        "            \n",
        "            # sum up batch loss\n",
        "            test_loss += loss_function(recon, data, mu, log_var).item()\n",
        "        \n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('====> Test set loss: {:.4f}'.format(test_loss))"
      ],
      "metadata": {
        "id": "Rwn_fCbKlPym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, num_epochs + 1):\n",
        "    train(epoch)\n",
        "    test()"
      ],
      "metadata": {
        "id": "lmJ7507mlQZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(vae, '20230416_VAE.pt')\n",
        "#vae = torch.load(path + '20230415_VAE.pt')"
      ],
      "metadata": {
        "id": "TXHA9UmyLWNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# store ground truth test images\n",
        "test_images_ground_truth, _ = next(iter(test_loader))\n",
        "test_images_ground_truth = test_images_ground_truth[-12:,:,:,:] #grab 12 last images"
      ],
      "metadata": {
        "id": "hAFYZQdwsacb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## now create an image of estimated images and ground truth\n",
        "# h/t https://stackoverflow.com/questions/66667949/pytorch-mnist-autoencoder-to-learn-10-digit-classification\n",
        "\n",
        "## run first five training images through the encoder\n",
        "### from https://github.com/dataflowr/notebooks/blob/master/HW3/VAE_clustering_empty.ipynb\n",
        "\n",
        "def show(img):\n",
        "    npimg = img.cpu().numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
        "    plt.suptitle('Test set reconstruction VAE & ConvNet VAE', fontsize=10, y=.67)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "def plot_reconstruction(vae, n=12):\n",
        "    \n",
        "    x = test_images_ground_truth\n",
        "    x = x[:n,:,:,:].to(device)\n",
        "    print(\"Ground truth images reformatted are of dimension\", x.size())\n",
        "    try:\n",
        "        out, _, _, log_p = vae(x.view(-1, image_size)) \n",
        "    except:\n",
        "        out, _, _ = vae(x.view(-1, image_size))\n",
        "    x_concat = torch.cat([x.view(-1, 1, 28, 28), out.view(-1, 1, 28, 28)], dim=0)\n",
        "    print(\"Concatenated object containing both ground truth & generic VAE is of size\", out.size())\n",
        "    ##\n",
        "    out, mu, logVAR = net(x)\n",
        "    \n",
        "    \n",
        "    x_concat = torch.cat([x_concat, out.view(-1, 1, 28, 28)], dim=0)\n",
        "    print(\"Concatenated object also containing ConvNet VAE is of size\", x_concat.size())\n",
        "\n",
        "    print(out.size())\n",
        "    print(out.type())\n",
        "    ##\n",
        "    out_grid = torchvision.utils.make_grid(x_concat, nrow=12)#.cpu().data\n",
        "    show(out_grid)\n",
        "    print(x_concat.size())\n",
        "\n",
        "plot_reconstruction(vae)"
      ],
      "metadata": {
        "id": "dAbGnY4mxOKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239) for MNIST\n",
        "(J. Ho, A. Jain, P. Abbeel 2020)\n",
        "\n",
        "![](https://raw.githubusercontent.com/dataflowr/website/master/modules/extras/diffusions/ddpm.png)\n",
        "\n",
        "\n",
        "Given a schedule $\\beta_1<\\beta_2<\\dots <\\beta_T$, the **forward diffusion process** is defined by:\n",
        "$q(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t}x_{t-1},\\beta_t I)$ and $q(x_{1:T}|x_0) = \\prod_{t=1}^T q(x_t|x_{t-1})$.\n",
        "\n",
        "With $\\alpha_t = 1-\\beta_t$ and $\\overline{\\alpha_t} = \\prod_{i=1}^t\\alpha_i$, we see that, with $\\epsilon\\sim\\mathcal{N}(0,I)$:\n",
        "\\begin{align*}\n",
        "x_t = \\sqrt{\\overline{\\alpha}_t}x_0 + \\sqrt{1-\\overline{\\alpha}_t}\\epsilon.\n",
        "\\end{align*}\n",
        "The law $q(x_{t-1}|x_t,\\epsilon)$ is explicit: $q(x_{t-1}|x_t,\\epsilon) = \\mathcal{N}(x_{t-1};\\mu(x_t,\\epsilon,t), \\gamma_t I)$ with,\n",
        "\\begin{align*}\n",
        "\\mu(x_t,\\epsilon, t) = \\frac{1}{\\sqrt{\\alpha_t}}\\left( x_t-\\frac{1-\\alpha_t}{\\sqrt{1-\\overline{\\alpha}_t}}\\epsilon\\right)\\text{ and, }\n",
        "\\gamma_t = \\frac{1-\\overline{\\alpha}_{t-1}}{1-\\overline{\\alpha}_{t}}\\beta_t\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "**Training**: to approximate **the reversed diffusion** $q(x_{t-1}|x_t)$ by a neural network given by $p_{\\theta}(x_{t-1}|x_t) = \\mathcal{N}(x_{t-1}; \\mu_{\\theta}(x_t,t), \\beta_t I)$ and $p(x_T) \\sim \\mathcal{N}(0,I)$, we maximize the usual Variational bound:\n",
        "\\begin{align*}\n",
        "\\mathbb{E}_{q(x_0)} \\ln p_{\\theta}(x_0) &\\geq L_T +\\sum_{t=2}^T L_{t-1}+L_0 \\text{ with, }L_{t-1} = \\mathbb{E}_q\\left[ \\frac{1}{2\\sigma_t^2}\\|\\mu_\\theta(x_t,t) -\\mu(x_t,\\epsilon,t)\\|^2\\right].\n",
        "\\end{align*}\n",
        "With the change of variable:\n",
        "\\begin{align*}\n",
        "\\mu_\\theta(x_t,t) = \\frac{1}{\\sqrt{\\alpha_t}}\\left( x_t-\\frac{1-\\alpha_t}{\\sqrt{1-\\overline{\\alpha}_t}}\\epsilon_\\theta(x_t,t)\\right),\n",
        "\\end{align*}\n",
        "ignoring the prefactor and sampling $\\tau$ instead of summing over all $t$, the loss is finally:\n",
        "\\begin{align*}\n",
        "\\ell(\\theta) = \\mathbb{E}_\\tau\\mathbb{E}_\\epsilon \\left[ \\|\\epsilon - \\epsilon_\\theta(\\sqrt{\\overline{\\alpha}_\\tau}x_0 + \\sqrt{1-\\overline{\\alpha}_\\tau}\\epsilon, \\tau)\\|^2\\right]\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "\n",
        "**Sampling**: to simulate the reversed diffusion with the learned $\\epsilon_\\theta(x_t,t)$ starting from $x_T\\sim \\mathcal{N}(0,I)$, iterate for $t=T,\\dots, 1$:\n",
        "\\begin{align*}\n",
        "x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}}\\left( x_t-\\frac{1-\\alpha_t}{\\sqrt{1-\\overline{\\alpha}_t}}\\epsilon_\\theta(x_t,t)\\right)+\\sqrt{\\beta_t}\\epsilon,\\text{ with } \\epsilon\\sim\\mathcal{N}(0,I).\n",
        "\\end{align*}"
      ],
      "metadata": {
        "id": "j3AWs4yCtc_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "6MB01CZZuKKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_images(images, title=\"\"):\n",
        "    \"\"\"Shows the provided images as sub-pictures in a square\"\"\"\n",
        "    images = [im.permute(1,2,0).numpy() for im in images]\n",
        "\n",
        "    # Defining number of rows and columns\n",
        "    fig = plt.figure(figsize=(8, 8))\n",
        "    rows = int(len(images) ** (1 / 2))\n",
        "    cols = round(len(images) / rows)\n",
        "\n",
        "    # Populating figure with sub-plots\n",
        "    idx = 0\n",
        "    for r in range(rows):\n",
        "        for c in range(cols):\n",
        "            fig.add_subplot(rows, cols, idx + 1)\n",
        "\n",
        "            if idx < len(images):\n",
        "                plt.imshow(images[idx], cmap=\"gray\")\n",
        "                plt.axis('off')\n",
        "                idx += 1\n",
        "    fig.suptitle(title, fontsize=30)\n",
        "    \n",
        "    # Showing the figure\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "D1Sswopa4zp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sinusoidal_embedding(n, d):\n",
        "    # Returns the standard positional embedding\n",
        "    embedding = torch.tensor([[i / 10_000 ** (2 * j / d) for j in range(d)] for i in range(n)])\n",
        "    sin_mask = torch.arange(0, n, 2)\n",
        "\n",
        "    embedding[sin_mask] = torch.sin(embedding[sin_mask])\n",
        "    embedding[1 - sin_mask] = torch.cos(embedding[sin_mask])\n",
        "\n",
        "    return embedding"
      ],
      "metadata": {
        "id": "Qv7b155x42RA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyConv(nn.Module):\n",
        "    def __init__(self, shape, in_c, out_c, kernel_size=3, stride=1, padding=1, activation=None, normalize=True):\n",
        "        super(MyConv, self).__init__()\n",
        "        self.ln = nn.LayerNorm(shape)\n",
        "        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size, stride, padding)\n",
        "        self.activation = nn.SiLU() if activation is None else activation\n",
        "        self.normalize = normalize\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.ln(x) if self.normalize else x\n",
        "        out = self.conv1(out)\n",
        "        out = self.activation(out)\n",
        "        return out\n",
        "    \n",
        "def MyTinyBlock(size, in_c, out_c):\n",
        "    return nn.Sequential(MyConv((in_c, size, size), in_c, out_c), \n",
        "                         MyConv((out_c, size, size), out_c, out_c), \n",
        "                         MyConv((out_c, size, size), out_c, out_c))\n",
        "\n",
        "def MyTinyUp(size, in_c):\n",
        "    return nn.Sequential(MyConv((in_c, size, size), in_c, in_c//2), \n",
        "                         MyConv((in_c//2, size, size), in_c//2, in_c//4), \n",
        "                         MyConv((in_c//4, size, size), in_c//4, in_c//4))"
      ],
      "metadata": {
        "id": "2H_n9wrz42zW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyTinyUNet(nn.Module):\n",
        "  # Here is a network with 3 down and 3 up with the tiny block\n",
        "    def __init__(self, in_c=1, out_c=1, size=32, n_steps=1000, time_emb_dim=100):\n",
        "        super(MyTinyUNet, self).__init__()\n",
        "\n",
        "        # Sinusoidal embedding\n",
        "        self.time_embed = nn.Embedding(n_steps, time_emb_dim)\n",
        "        self.time_embed.weight.data = sinusoidal_embedding(n_steps, time_emb_dim)\n",
        "        self.time_embed.requires_grad_(False)\n",
        "\n",
        "        # First half\n",
        "        self.te1 = self._make_te(time_emb_dim, 1)\n",
        "        self.b1 = MyTinyBlock(size, in_c, 10)\n",
        "        self.down1 = nn.Conv2d(10, 10, 4, 2, 1)\n",
        "        self.te2 = self._make_te(time_emb_dim, 10)\n",
        "        self.b2 = MyTinyBlock(size//2, 10, 20)\n",
        "        self.down2 = nn.Conv2d(20, 20, 4, 2, 1)\n",
        "        self.te3 = self._make_te(time_emb_dim, 20)\n",
        "        self.b3 = MyTinyBlock(size//4, 20, 40)\n",
        "        self.down3 = nn.Conv2d(40, 40, 4, 2, 1)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.te_mid = self._make_te(time_emb_dim, 40)\n",
        "        self.b_mid = nn.Sequential(\n",
        "            MyConv((40, size//8, size//8), 40, 20),\n",
        "            MyConv((20, size//8, size//8), 20, 20),\n",
        "            MyConv((20, size//8, size//8), 20, 40)\n",
        "        )\n",
        "\n",
        "        # Second half\n",
        "        self.up1 = nn.ConvTranspose2d(40, 40, 4, 2, 1)\n",
        "        self.te4 = self._make_te(time_emb_dim, 80)\n",
        "        self.b4 = MyTinyUp(size//4, 80)\n",
        "        self.up2 = nn.ConvTranspose2d(20, 20, 4, 2, 1)\n",
        "        self.te5 = self._make_te(time_emb_dim, 40)\n",
        "        self.b5 = MyTinyUp(size//2, 40)\n",
        "        self.up3 = nn.ConvTranspose2d(10, 10, 4, 2, 1)\n",
        "        self.te_out = self._make_te(time_emb_dim, 20)\n",
        "        self.b_out = MyTinyBlock(size, 20, 10)\n",
        "        self.conv_out = nn.Conv2d(10, out_c, 3, 1, 1)\n",
        "\n",
        "    def forward(self, x, t): # x is (bs, in_c, size, size) t is (bs)\n",
        "        t = self.time_embed(t)\n",
        "        n = len(x)\n",
        "        out1 = self.b1(x + self.te1(t).reshape(n, -1, 1, 1))  # (bs, 10, size/2, size/2)\n",
        "        out2 = self.b2(self.down1(out1) + self.te2(t).reshape(n, -1, 1, 1))  # (bs, 20, size/4, size/4)\n",
        "        out3 = self.b3(self.down2(out2) + self.te3(t).reshape(n, -1, 1, 1))  # (bs, 40, size/8, size/8)\n",
        "\n",
        "        out_mid = self.b_mid(self.down3(out3) + self.te_mid(t).reshape(n, -1, 1, 1))  # (bs, 40, size/8, size/8)\n",
        "\n",
        "        out4 = torch.cat((out3, self.up1(out_mid)), dim=1)  # (bs, 80, size/8, size/8)\n",
        "        out4 = self.b4(out4 + self.te4(t).reshape(n, -1, 1, 1))  # (bs, 20, size/8, size/8)\n",
        "        out5 = torch.cat((out2, self.up2(out4)), dim=1)  # (bs, 40, size/4, size/4)\n",
        "        out5 = self.b5(out5 + self.te5(t).reshape(n, -1, 1, 1))  # (bs, 10, size/2, size/2)\n",
        "        out = torch.cat((out1, self.up3(out5)), dim=1)  # (bs, 20, size, size)\n",
        "        out = self.b_out(out + self.te_out(t).reshape(n, -1, 1, 1))  # (bs, 10, size, size)\n",
        "        out = self.conv_out(out) # (bs, out_c, size, size)\n",
        "        return out\n",
        "\n",
        "    def _make_te(self, dim_in, dim_out):\n",
        "        return nn.Sequential(nn.Linear(dim_in, dim_out), nn.SiLU(), nn.Linear(dim_out, dim_out))"
      ],
      "metadata": {
        "id": "EiGggoHH46UI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bs = 3\n",
        "x = torch.randn(bs,1,32,32)\n",
        "n_steps=1000\n",
        "timesteps = torch.randint(0, n_steps, (bs,)).long()\n",
        "unet = MyTinyUNet(in_c =1, out_c =1, size=32)"
      ],
      "metadata": {
        "id": "TcHLTuZs491H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = unet(x,timesteps)\n",
        "y.shape"
      ],
      "metadata": {
        "id": "AkzV3d6K5Bqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DDPM(nn.Module):\n",
        "    def __init__(self, network, num_timesteps, beta_start=0.0001, beta_end=0.02, device=device) -> None:\n",
        "        super(DDPM, self).__init__()\n",
        "        self.num_timesteps = num_timesteps\n",
        "        self.betas = torch.linspace(beta_start, beta_end, num_timesteps, dtype=torch.float32).to(device)\n",
        "        self.alphas = 1.0 - self.betas\n",
        "        self.alphas_cumprod = torch.cumprod(self.alphas, axis=0)\n",
        "        self.network = network\n",
        "        self.device = device\n",
        "        self.sqrt_alphas_cumprod = self.alphas_cumprod ** 0.5 # used in add_noise\n",
        "        self.sqrt_one_minus_alphas_cumprod = (1 - self.alphas_cumprod) ** 0.5 # used in add_noise and step\n",
        "\n",
        "    def add_noise(self, x_start, x_noise, timesteps):\n",
        "        # The forward process\n",
        "        # x_start and x_noise (bs, n_c, w, d)\n",
        "        # timesteps (bs)\n",
        "        s1 = self.sqrt_alphas_cumprod[timesteps] # bs\n",
        "        s2 = self.sqrt_one_minus_alphas_cumprod[timesteps] # bs\n",
        "        s1 = s1.reshape(-1,1,1,1) # (bs, 1, 1, 1) for broadcasting\n",
        "        s2 = s2.reshape(-1,1,1,1) # (bs, 1, 1, 1)\n",
        "        return s1 * x_start + s2 * x_noise\n",
        "\n",
        "    def reverse(self, x, t):\n",
        "        # The network return the estimation of the noise we added\n",
        "        return self.network(x, t)\n",
        "    \n",
        "    def step(self, model_output, timestep, sample):\n",
        "        # one step of sampling\n",
        "        # timestep (1)\n",
        "        t = timestep\n",
        "        coef_epsilon = (1-self.alphas)/self.sqrt_one_minus_alphas_cumprod\n",
        "        coef_eps_t = coef_epsilon[t].reshape(-1,1,1,1)\n",
        "        coef_first = 1/self.alphas ** 0.5\n",
        "        coef_first_t = coef_first[t].reshape(-1,1,1,1)\n",
        "        pred_prev_sample = coef_first_t*(sample-coef_eps_t*model_output)\n",
        "\n",
        "        variance = 0\n",
        "        if t > 0:\n",
        "            noise = torch.randn_like(model_output).to(self.device)\n",
        "            variance = ((self.betas[t] ** 0.5) * noise)\n",
        "            \n",
        "        pred_prev_sample = pred_prev_sample + variance\n",
        "\n",
        "        return pred_prev_sample"
      ],
      "metadata": {
        "id": "H3THr3215CJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_timesteps = 1000\n",
        "betas = torch.linspace(0.0001, 0.02, num_timesteps, dtype=torch.float32).to(device)"
      ],
      "metadata": {
        "id": "_AwkImC65GO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "betas[timesteps]"
      ],
      "metadata": {
        "id": "bgr-Umab5G1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "betas[10]"
      ],
      "metadata": {
        "id": "QcXUtiAw5KzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "betas[timesteps].reshape(-1,1,1,1).shape"
      ],
      "metadata": {
        "id": "U0D5vxS85MxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network = MyTinyUNet(in_c =1, out_c =1, size=32)\n",
        "model = DDPM(network, num_timesteps, beta_start=0.0001, beta_end=0.02, device=device)"
      ],
      "metadata": {
        "id": "zKy8r0b35PHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bs = 5\n",
        "x = torch.randn(bs,1,32,32).to(device)\n",
        "timesteps = 10*torch.ones(bs,).long().long().to(device)"
      ],
      "metadata": {
        "id": "QYzBel6p5Php"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timesteps.shape"
      ],
      "metadata": {
        "id": "6Y3wgiGx5Rdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = model.add_noise(x,x,timesteps)\n",
        "y.shape"
      ],
      "metadata": {
        "id": "nQdZixkS5XMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = model.step(x,timesteps[0],x)\n",
        "y.shape"
      ],
      "metadata": {
        "id": "fvAqNm5v5bM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can check that all the parameters of the UNet network are indeed parameters of the DDPM model like this:"
      ],
      "metadata": {
        "id": "ysEcs_Fc5eXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for n, p in model.named_parameters():\n",
        "    print(n, p.shape)"
      ],
      "metadata": {
        "id": "pQScyX495fMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(model, dataloader, optimizer, num_epochs, num_timesteps, device=device):\n",
        "    \"\"\"Training loop for DDPM\"\"\"\n",
        "\n",
        "    global_step = 0\n",
        "    losses = []\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        progress_bar = tqdm(total=len(dataloader))\n",
        "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
        "        for step, batch in enumerate(dataloader):\n",
        "            batch = batch[0].to(device)\n",
        "            noise = torch.randn(batch.shape).to(device)\n",
        "            timesteps = torch.randint(0, num_timesteps, (batch.shape[0],)).long().to(device)\n",
        "\n",
        "            noisy = model.add_noise(batch, noise, timesteps)\n",
        "            noise_pred = model.reverse(noisy, timesteps)\n",
        "            loss = F.mse_loss(noise_pred, noise)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            progress_bar.update(1)\n",
        "            logs = {\"loss\": loss.detach().item(), \"step\": global_step}\n",
        "            losses.append(loss.detach().item())\n",
        "            progress_bar.set_postfix(**logs)\n",
        "            global_step += 1\n",
        "        \n",
        "        progress_bar.close()"
      ],
      "metadata": {
        "id": "1mPyGVwQ5h3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir = './data/'\n",
        "transform01 = torchvision.transforms.Compose([\n",
        "        torchvision.transforms.Resize(32),\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize((0.5), (0.5))\n",
        "    ])\n",
        "dataset = torchvision.datasets.MNIST(root=root_dir, train=True, transform=transform01, download=True)\n",
        "dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=4096, shuffle=True, num_workers=10)"
      ],
      "metadata": {
        "id": "dCPEsBce5kEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for b in dataloader:\n",
        "    batch = b[0]\n",
        "    break\n",
        "\n",
        "bn = [b for b in batch[:100]] \n",
        "show_images(bn, \"origin\")"
      ],
      "metadata": {
        "id": "pNH9KpU65oVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "num_epochs = 50\n",
        "num_timesteps = 1000\n",
        "network = MyTinyUNet()\n",
        "network = network.to(device)\n",
        "model = DDPM(network, num_timesteps, beta_start=0.0001, beta_end=0.02, device=device)\n",
        "optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate)\n",
        "training_loop(model, dataloader, optimizer, num_epochs, num_timesteps, device=device)        "
      ],
      "metadata": {
        "id": "qG2L6O6L5quP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.save(model, '20230415_Diffusion.pt')\n",
        "model = torch.load(path + '20230416_Diffusion.pt')"
      ],
      "metadata": {
        "id": "kGf-u0qp4Ya5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_image(ddpm, sample_size, channel, size):\n",
        "    \"\"\"Generate the image from the Gaussian noise\"\"\"\n",
        "\n",
        "    frames = []\n",
        "    frames_mid = []\n",
        "    ddpm.eval()\n",
        "    with torch.no_grad():\n",
        "        timesteps = list(range(ddpm.num_timesteps))[::-1]\n",
        "        sample = torch.randn(sample_size, channel, size, size).to(device)\n",
        "        \n",
        "        for i, t in enumerate(tqdm(timesteps)):\n",
        "            time_tensor = (torch.ones(sample_size,1) * t).long().to(device)\n",
        "            residual = ddpm.reverse(sample, time_tensor)\n",
        "            sample = ddpm.step(residual, time_tensor[0], sample)\n",
        "\n",
        "            if t==500:\n",
        "                for i in range(sample_size):\n",
        "                    frames_mid.append(sample[i].detach().cpu())\n",
        "\n",
        "        for i in range(sample_size):\n",
        "            frames.append(sample[i].detach().cpu())\n",
        "    return frames, frames_mid"
      ],
      "metadata": {
        "id": "eQXWsjaG5s6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated, generated_mid = generate_image(model, 100, 1, 32)"
      ],
      "metadata": {
        "id": "jU97qOJi5vDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_images(generated_mid, \"Mid result\")\n",
        "show_images(generated, \"Final result\")"
      ],
      "metadata": {
        "id": "rcQMvn2s5xM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rescale(x):\n",
        "    return (x+1)/2\n",
        "\n",
        "def show_images_rescale(images, title=\"\"):\n",
        "    \"\"\"Shows the provided images as sub-pictures in a square\"\"\"\n",
        "    images = [rescale((im.permute(1,2,0)).numpy()) for im in images]\n",
        "\n",
        "    # Defining number of rows and columns\n",
        "    fig = plt.figure(figsize=(8, 8))\n",
        "    rows = int(len(images) ** (1 / 2))\n",
        "    cols = round(len(images) / rows)\n",
        "\n",
        "    # Populating figure with sub-plots\n",
        "    idx = 0\n",
        "    for r in range(rows):\n",
        "        for c in range(cols):\n",
        "            fig.add_subplot(rows, cols, idx + 1)\n",
        "\n",
        "            if idx < len(images):\n",
        "                #plt.imshow(images[idx].reshape(pixel, pixel, n_channels), cmap=\"gray\")\n",
        "                plt.imshow(images[idx])\n",
        "                plt.axis('off')\n",
        "                idx += 1\n",
        "    fig.suptitle(title, fontsize=30)\n",
        "    \n",
        "    # Showing the figure\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "jt0gDAhM5zaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_images_rescale(generated, \"Final result\")"
      ],
      "metadata": {
        "id": "n_0cZ6Ws5119"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gHaH4BYv4gvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Probabilistic PCA"
      ],
      "metadata": {
        "id": "BE0oQUN9RBss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generic PCA Elbow Plot"
      ],
      "metadata": {
        "id": "mIxG_ctjeRf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "KPZw3mKURDvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## from PCA\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "testset = datasets.MNIST(root='./data', train=False, download=False, transform=transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "_cGIVqHLRH7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "### from generic VA\n",
        "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
        "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False)\n",
        "'''"
      ],
      "metadata": {
        "id": "Vqfv9jmmr5-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_matrix(dataloader):\n",
        "    data_matrix = []\n",
        "    for i, (images, labels) in enumerate(dataloader):\n",
        "        images = images.view(-1, 28 * 28)\n",
        "        data_matrix.append(images)\n",
        "    return torch.vstack(data_matrix)\n",
        "\n",
        "train_data_matrix = get_data_matrix(trainloader)\n",
        "test_data_matrix = get_data_matrix(testloader)"
      ],
      "metadata": {
        "id": "pTtUQg03ROR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA()\n",
        "pca.fit(train_data_matrix.numpy())\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n"
      ],
      "metadata": {
        "id": "3q6HfmmYcgD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_elbow_curve(explained_variance_ratio, max_components=200):\n",
        "    cum_explained_variance = np.cumsum(explained_variance_ratio[:max_components])\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(1, max_components + 1), cum_explained_variance)\n",
        "    plt.xlabel('Number of Components')\n",
        "    plt.ylabel('Cumulative Explained Variance')\n",
        "    plt.title('Elbow Plot')\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "plot_elbow_curve(explained_variance_ratio)"
      ],
      "metadata": {
        "id": "4d1wt2HZeciV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_matrix.size()\n"
      ],
      "metadata": {
        "id": "5HakhJ4XRalN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProbabilisticPCA(torch.nn.Module):\n",
        "    def __init__(self, n_components):\n",
        "        super(ProbabilisticPCA, self).__init__()\n",
        "        self.W = torch.nn.Parameter(torch.randn(28 * 28, n_components) * 0.01, requires_grad=True)\n",
        "        self.mu = torch.nn.Parameter(torch.zeros(1, 28 * 28), requires_grad=True)\n",
        "\n",
        "    def forward(self, X):\n",
        "        return (X - self.mu) @ self.W\n",
        "\n",
        "    def reconstruct(self, Z):\n",
        "        return Z @ self.W.t() + self.mu"
      ],
      "metadata": {
        "id": "ggQ4eapIRiG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_probabilistic_pca(train_data, n_components, learning_rate, num_epochs):\n",
        "    model = ProbabilisticPCA(n_components)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        Z = model(train_data)\n",
        "        reconstruction = model.reconstruct(Z)\n",
        "        loss = torch.mean(torch.square(train_data - reconstruction))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "CmtMlhiYZgSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_components = 100\n",
        "learning_rate = 1e-3\n",
        "num_epochs = 250\n",
        "\n",
        "model = train_probabilistic_pca(train_data_matrix, n_components, learning_rate, num_epochs)\n",
        "train_data_reduced = model(train_data_matrix).detach().numpy()"
      ],
      "metadata": {
        "id": "ye1bpHpuZk6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the labels for the training data\n",
        "train_labels = trainset.targets.numpy()\n",
        "\n",
        "test_data_reduced = model(test_data_matrix).detach().numpy()\n",
        "\n",
        "# Get the labels for the test data\n",
        "test_labels = testset.targets.numpy()"
      ],
      "metadata": {
        "id": "RXskatn4ZoZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels.shape\n"
      ],
      "metadata": {
        "id": "aqV3WPaU3F9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## h/t https://stackoverflow.com/questions/36522220/searching-a-sequence-in-a-numpy-array\n",
        "\n",
        "def search_sequence_numpy(arr,seq):\n",
        "    \"\"\" Find sequence in an array using NumPy only.\n",
        "\n",
        "    Parameters\n",
        "    ----------    \n",
        "    arr    : input 1D array\n",
        "    seq    : input 1D array\n",
        "\n",
        "    Output\n",
        "    ------    \n",
        "    Output : 1D Array of indices in the input array that satisfy the \n",
        "    matching of input sequence in the input array.\n",
        "    In case of no match, an empty list is returned.\n",
        "    \"\"\"\n",
        "\n",
        "    # Store sizes of input array and sequence\n",
        "    Na, Nseq = arr.size, seq.size\n",
        "\n",
        "    # Range of sequence\n",
        "    r_seq = np.arange(Nseq)\n",
        "\n",
        "    # Create a 2D array of sliding indices across the entire length of input array.\n",
        "    # Match up with the input sequence & get the matching starting indices.\n",
        "    M = (arr[np.arange(Na-Nseq+1)[:,None] + r_seq] == seq).all(1)\n",
        "\n",
        "    # Get the range of those indices as final output\n",
        "    if M.any() >0:\n",
        "        return np.where(np.convolve(M,np.ones((Nseq),dtype=int))>0)[0]\n",
        "    else:\n",
        "        return []         # No match found"
      ],
      "metadata": {
        "id": "QsQyUUOo4I4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr = test_labels\n",
        "seq = np.array([4,4,9,2,5,4,7,6,7,9,0,5])\n",
        "search_sequence_numpy(arr,seq)"
      ],
      "metadata": {
        "id": "ASg2kWTI4O77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_reduced_data(data, labels, title):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    scatter = plt.scatter(data[:, 0], data[:, 1], c=labels, cmap='tab10', s=10, alpha=0.8)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Component 1')\n",
        "    plt.ylabel('Component 2')\n",
        "    plt.colorbar(scatter)\n",
        "    plt.show()\n",
        "\n",
        "# Plot the reduced training data\n",
        "plot_reduced_data(train_data_reduced, train_labels, 'Probabilistic PCA of MNIST Train Data')"
      ],
      "metadata": {
        "id": "Tc_bc9Zd23nD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the reduced test data\n",
        "plot_reduced_data(test_data_reduced, test_labels, 'Probabilistic PCA of MNIST Test Data')"
      ],
      "metadata": {
        "id": "-GvqAvcGak4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reconstructed_test_data = model.reconstruct(torch.tensor(test_data_reduced)).detach().numpy()\n"
      ],
      "metadata": {
        "id": "D23N2kIHbXgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_reconstructed_images(test_data, reconstructed_data, labels, num_samples=5):\n",
        "    idx = np.random.choice(test_data.shape[0], num_samples, replace=False)\n",
        "    original_images = test_data[idx].reshape(-1, 28, 28)\n",
        "    reconstructed_images = reconstructed_data[idx].reshape(-1, 28, 28)\n",
        "    label_samples = labels[idx]\n",
        "\n",
        "    fig, axes = plt.subplots(nrows=2, ncols=num_samples, figsize=(2*num_samples, 4))\n",
        "\n",
        "    for i, ax in enumerate(axes[0]):\n",
        "        ax.imshow(original_images[i], cmap='gray')\n",
        "        ax.set_title(f'Label: {label_samples[i]}')\n",
        "        ax.axis('off')\n",
        "\n",
        "    for i, ax in enumerate(axes[1]):\n",
        "        ax.imshow(reconstructed_images[i], cmap='gray')\n",
        "        ax.set_title('Reconstructed')\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "HhBgHEDmb51W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_reconstructed_images(test_data_matrix.numpy(), reconstructed_test_data, test_labels)"
      ],
      "metadata": {
        "id": "9p8eWj3AcbR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test data plot"
      ],
      "metadata": {
        "id": "782WaooQUHhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#original_images = test_data[idx].reshape(-1, 28, 28)\n",
        "\n",
        "reconstructed_test_data = model.reconstruct(torch.tensor(test_data_reduced))\n",
        "print(reconstructed_test_data.size())\n",
        "print(\"The size of the test set is: \" + str(reconstructed_test_data.size()))\n",
        "#reconstructed_test_data = reconstructed_test_data[-12:,:]\n",
        "reconstructed_test_data = reconstructed_test_data[116:128:,:]\n",
        "print(reconstructed_test_data.size())"
      ],
      "metadata": {
        "id": "arGA5sgCej39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## now create an image of estimated images and ground truth\n",
        "# h/t https://stackoverflow.com/questions/66667949/pytorch-mnist-autoencoder-to-learn-10-digit-classification\n",
        "\n",
        "## run first five training images through the encoder\n",
        "### from https://github.com/dataflowr/notebooks/blob/master/HW3/VAE_clustering_empty.ipynb\n",
        "\n",
        "def show(img):\n",
        "    npimg = img.cpu().numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
        "    plt.suptitle('Test set reconstruction VAE & ConvNet VAE', fontsize=10, y=.67)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "def plot_reconstruction(vae, n=12):\n",
        "    \n",
        "    x = test_images_ground_truth\n",
        "    x = x[:n,:,:,:].to(device)\n",
        "    print(\"Ground truth images reformatted are of dimension\", x.size())\n",
        "    try:\n",
        "        out, _, _, log_p = vae(x.view(-1, image_size)) \n",
        "    except:\n",
        "        out, _, _ = vae(x.view(-1, image_size))\n",
        "    x_concat = torch.cat([x.view(-1, 1, 28, 28), out.view(-1, 1, 28, 28)], dim=0)\n",
        "    print(\"Concatenated object containing both ground truth & generic VAE is of size\", out.size())\n",
        "    ## Conv Net\n",
        "    out, mu, logVAR = net(x)\n",
        "    x_concat = torch.cat([x_concat, out.view(-1, 1, 28, 28)], dim=0)\n",
        "    ## Prob PCA\n",
        "    x_concat = torch.cat([x_concat, reconstructed_test_data.view(-1, 1, 28, 28).to(device)], dim=0)\n",
        "\n",
        "    \n",
        "    \n",
        "    print(\"Concatenated object also containing ConvNet VAE is of size\", x_concat.size())\n",
        "\n",
        "    print(out.size())\n",
        "    print(out.type())\n",
        "    ##\n",
        "    out_grid = torchvision.utils.make_grid(x_concat, nrow=12)#.cpu().data\n",
        "    show(out_grid)\n",
        "    print(x_concat.size())\n",
        "\n",
        "plot_reconstruction(vae)"
      ],
      "metadata": {
        "id": "Bkwi3Nc0ehvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "hHmMRLJLWFOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m6o6Mf1WpVr4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}